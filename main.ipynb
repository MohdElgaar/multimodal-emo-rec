{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from model import MyModel\n",
    "from data import MyDataset, Collates\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "from torch_future import OneCycleLR, Nadam, SGDW\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from data import emotions\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "importlib.reload(data)\n",
    "from data import MyDataset, Collates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331/331 [01:56<00:00,  2.83it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "collates = Collates()\n",
    "\n",
    "train_path = '/usr/cs/public/mohd/data/train'\n",
    "train_txt = '/usr/cs/public/mohd/train_data_labeled.txt'\n",
    "train_dataset = MyDataset(train_path, train_txt, size=500)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "        collate_fn = collates.collate_fn, num_workers=0)\n",
    "\n",
    "val_path = '/usr/cs/public/mohd/data/val'\n",
    "val_txt = '/usr/cs/public/mohd/val_data_labeled.txt'\n",
    "val_dataset = MyDataset(val_path, val_txt, mode='val', size=-1)\n",
    "val_dataset.update_data()\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "        collate_fn = collates.val_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(train_dataset):\n",
    "    n = item[2].shape[0]\n",
    "    if n < 1 :\n",
    "        print(n, i, train_dataset.imgs['dbpGH5iP0GE'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MyDataset(val_path, val_txt, mode='val', size=2500)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "        collate_fn = collates.val_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(torch.cuda)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_train = (train_dataset.imgs, train_dataset.audios, train_dataset.texts, train_dataset.files)\n",
    "saved_val = (val_dataset.imgs, val_dataset.audios, val_dataset.texts, val_dataset.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.imgs = saved_train[0]\n",
    "train_dataset.audios = saved_train[1]\n",
    "train_dataset.texts = saved_train[2]\n",
    "train_dataset.files = saved_train[3]\n",
    "val_dataset.imgs = saved_val[0]\n",
    "val_dataset.audios = saved_val[1]\n",
    "val_dataset.texts = saved_val[2]\n",
    "val_dataset.files = saved_val[3]\n",
    "train_dataset.size = len(train_dataset.texts)\n",
    "val_dataset.size = len(val_dataset.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_avg(preds):\n",
    "    if preds and len(preds) > 0:\n",
    "        return np.expand_dims(np.mean(np.concatenate(preds),0),0)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def combine_voting(preds):\n",
    "    preds = np.argmax(np.concatenate(preds), 1)\n",
    "    ind = np.argmax(np.bincount(preds))\n",
    "    return ind\n",
    "\n",
    "mean = lambda l: sum(l)/len(l)\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience = 20, delta = 0.01):\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        \n",
    "    def __call__(self, loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = loss\n",
    "        elif loss > self.best_loss + self.delta:\n",
    "            self.counter += 1\n",
    "        elif loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            self.counter = 0\n",
    "        \n",
    "        if self.counter >= self.patience:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "def initiate_model(hparams, name, resume=None):\n",
    "    import importlib\n",
    "    import model\n",
    "    import torch\n",
    "    importlib.reload(torch)\n",
    "    importlib.reload(model)\n",
    "    from model import MyModel\n",
    "    model = MyModel(name, hparams).to(device)\n",
    "\n",
    "    if resume is not None:\n",
    "      state = torch.load(os.path.join('/usr/cs/grad/doc/melgaar/multimodal-emo-rec/ckpt','%s_%d.pt'%resume))\n",
    "    \n",
    "      model.load_state_dict(state['model_state_dict'])\n",
    "      model.name = state['name']\n",
    "      model.iter = state['iter']\n",
    "      model.stage = state['stage']\n",
    "      writer = SummaryWriter(logdir='runs/' + model.name, purge_step = model.iter)\n",
    "      print(\"Loaded model\")\n",
    "    else:\n",
    "      writer = SummaryWriter(logdir='runs/' + name)\n",
    "    \n",
    "    return model, writer\n",
    "\n",
    "def initiate_optimizers(hparams, model):\n",
    "#     spk_params = list(model.classify_spk.parameters()) \\\n",
    "#         + list(model.classify_gen.parameters()) \\\n",
    "#         + list(model.classify_age.parameters()) \\\n",
    "#         + list(model.av_spk.parameters())\n",
    "    audio_params = list(model.rnn_audio.parameters())\n",
    "    img_params = list(model.rnn_img.parameters())\n",
    "    text_params = list(model.rnn_text.parameters())\n",
    "    av_params = list(model.av.parameters())\n",
    "    at_params = list(model.at.parameters())\n",
    "    vt_params = list(model.vt.parameters()) \n",
    "    int_params = list(model.avt.parameters()) + list(model.classify_integrated.parameters())\n",
    "\n",
    "    if hparams.setting == 'aux':\n",
    "        audio_params += list(model.classify_audio.parameters())\n",
    "        img_params += list(model.classify_img.parameters())\n",
    "        text_params += list(model.classify_text.parameters())\n",
    "        av_params += list(model.classify_av.parameters())\n",
    "        at_params += list(model.classify_at.parameters())\n",
    "        vt_params += list(model.classify_vt.parameters())\n",
    "    \n",
    "    if hparams.tune_prev:\n",
    "        int_params += (audio_params + img_params + text_params + av_params + at_params + vt_params)\n",
    "        av_params += (audio_params + img_params)\n",
    "        at_params += (audio_params + text_params)\n",
    "        vt_params += (img_params + text_params)\n",
    "    \n",
    "    optims = {'adam':optim.Adam,\n",
    "             'nadam':Nadam,\n",
    "             'adamw': optim.AdamW,\n",
    "             'sgd': lambda params, lr, weight_decay: optim.SGD(params,\n",
    "                                                               lr,\n",
    "                                                               weight_decay=weight_decay,\n",
    "                                                               momentum=0.9, nesterov=True),\n",
    "             'sgdw':lambda params, lr, weight_decay: SGDW(params,\n",
    "                                                          lr,\n",
    "                                                          weight_decay=weight_decay,\n",
    "                                                          momentum=0.9, nesterov=True)}\n",
    "        \n",
    "    optimizers = {}\n",
    "    optimizers['audio'] = optims[hparams.audio_optim](audio_params, lr = hparams.audio_lr, weight_decay = hparams.audio_reg)\n",
    "    optimizers['img'] = optims[hparams.img_optim](img_params, lr = hparams.img_lr,weight_decay = hparams.img_reg)\n",
    "    optimizers['text'] = optims[hparams.text_optim](text_params, lr = hparams.text_lr, weight_decay = hparams.text_reg)\n",
    "    optimizers['av'] = optims[hparams.av_optim](av_params, lr = hparams.av_lr, weight_decay = hparams.av_reg)\n",
    "    optimizers['at'] = optims[hparams.at_optim](at_params, lr = hparams.at_lr, weight_decay = hparams.at_reg)\n",
    "    optimizers['vt'] = optims[hparams.vt_optim](vt_params, lr = hparams.vt_lr, weight_decay = hparams.vt_reg)\n",
    "    optimizers['int'] = optims[hparams.int_optim](int_params, lr = hparams.int_lr, weight_decay = hparams.int_reg)\n",
    "#     optimizers['spk'] = optims[hparams.spk_optim](spk_params, lr = hparams.spk_lr, weight_decay = hparams.spk_reg)\n",
    "    return optimizers\n",
    "\n",
    "def save_model(model):\n",
    "    state = {'model_state_dict': model.state_dict(),\n",
    "             'name': model.name,\n",
    "             'iter': model.iter,\n",
    "             'stage': model.stage\n",
    "            }\n",
    "    torch.save(state, os.path.join('/usr/cs/grad/doc/melgaar/multimodal-emo-rec/ckpt', \"%s_%d.pt\"%(model.name,\n",
    "                                                                                                  model.iter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_fn(pred, label):\n",
    "    pred[pred > 0.5] = 1\n",
    "    pred[pred <= 0.5] = 0\n",
    "    pred = pred.detach().cpu()\n",
    "    label = label.detach().cpu()\n",
    "    acc = (pred == label).numpy().mean()\n",
    "    \n",
    "    f1s, waccs = [], []\n",
    "    for emo in range(label.shape[1]):\n",
    "        sublabels = label[:,emo]\n",
    "        subpreds = pred[:,emo]\n",
    "        f1 = f1_score(sublabels, subpreds, average='binary')\n",
    "        counts = Counter(sublabels.tolist())\n",
    "        n, p = counts[0], counts[1]\n",
    "        tn, fp, fn, tp = confusion_matrix(sublabels, subpreds).ravel()\n",
    "        wacc = (tp * n/p + tn) / (2*n)\n",
    "        f1s.append(f1)\n",
    "        waccs.append(wacc)\n",
    "    \n",
    "\n",
    "    return acc, mean(f1s), mean(waccs)\n",
    "\n",
    "\n",
    "def train_step(model, optimizers, batch, writer):\n",
    "    bs = batch[0].shape[0]\n",
    "\n",
    "    if model.hparams.masking:\n",
    "        #text\n",
    "        mask = torch.ones((bs,1,1), device=batch[0].device)\n",
    "        text_mask = F.normalize(F.dropout(mask, model.hparams.mask_rate))\n",
    "        batch[1] *= text_mask\n",
    "\n",
    "        #audio\n",
    "        mask = torch.ones((bs,1,1), device=batch[0].device)\n",
    "        audio_mask = F.normalize(F.dropout(mask, model.hparams.mask_rate))\n",
    "        batch[0] *= audio_mask\n",
    "\n",
    "        #imgs\n",
    "        mask = torch.ones((bs,1,1), device=batch[0].device)\n",
    "        imgs_mask = F.normalize(F.dropout(mask, model.hparams.mask_rate))\n",
    "        batch[3] *= imgs_mask\n",
    "\n",
    "    results = model(batch[:6])\n",
    "    y_int = batch[6]\n",
    "\n",
    "#     if masking and model.stage >= 2:\n",
    "#         loss_audio = (model.loss(out_audio, y_audio)[audio_mask.squeeze() != 0]).mean()\n",
    "#         loss_imgs = (model.loss(out_imgs, y_imgs)[imgs_mask.squeeze() != 0]).mean()\n",
    "#         loss_text = (model.loss(out_text, y_int)[text_mask.squeeze() != 0]).mean()\n",
    "\n",
    "    if model.stage == 1:\n",
    "        if model.hparams.setting == 'aux':\n",
    "            out_audio, out_imgs, out_text = results\n",
    "#             loss_spk = model.hparams.alpha_spk * model.loss(out_spk, spk).mean()\n",
    "#             loss_gen = model.hparams.alpha_gen * model.loss(out_gen, gen).mean()\n",
    "#             loss_age = model.hparams.alpha_age * model.reg_loss(out_age, age).mean()\n",
    "            loss_audio = model.loss(out_audio, y_int).mean()\n",
    "            loss_imgs = model.loss(out_imgs, y_int).mean()\n",
    "            loss_text = model.loss(out_text, y_int).mean()\n",
    "            loss = (loss_audio + loss_imgs + loss_text)/3\n",
    "#             loss += loss_spk + loss_gen + loss_age\n",
    "        elif model.hparams.setting == 'full':\n",
    "            out_int = results\n",
    "            loss_int = model.loss(out_int, y_int).mean()\n",
    "#             loss_spk = model.hparams.alpha_spk * model.loss(out_spk, spk).mean()\n",
    "#             loss_gen = model.hparams.alpha_gen * model.loss(out_gen, gen).mean()\n",
    "#             loss_age = model.hparams.alpha_age * model.reg_loss(out_age, age).mean()\n",
    "            loss = loss_int\n",
    "        \n",
    "        optimizers['audio'].zero_grad()\n",
    "        optimizers['img'].zero_grad()\n",
    "        optimizers['text'].zero_grad()\n",
    "#         optimizers['spk'].zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), model.hparams.clip)\n",
    "        if not model.earlystop_audio:\n",
    "            optimizers['audio'].step()\n",
    "            if model.hparams.setting == 'aux':\n",
    "                audio_acc, audio_f1, audio_wacc = acc_fn(out_audio, y_int)\n",
    "                writer.add_scalar('train/stage1/loss/audio_loss', loss_audio, model.iter)\n",
    "                writer.add_scalar('train/stage1/acc/audio_acc',audio_acc, model.iter)\n",
    "        if not model.earlystop_img:\n",
    "            optimizers['img'].step()\n",
    "            if model.hparams.setting == 'aux':\n",
    "                img_acc, img_f1, img_wacc = acc_fn(out_imgs, y_int)\n",
    "                writer.add_scalar('train/stage1/loss/img_loss', loss_imgs, model.iter)\n",
    "                writer.add_scalar('train/stage1/acc/img_acc',img_acc, model.iter)\n",
    "        if not model.earlystop_text:\n",
    "            optimizers['text'].step()\n",
    "            if model.hparams.setting == 'aux':\n",
    "                text_acc, text_f1, text_wacc  = acc_fn(out_text, y_int)\n",
    "                writer.add_scalar('train/stage1/loss/text_loss', loss_text, model.iter)\n",
    "                writer.add_scalar('train/stage1/acc/text_acc',text_acc, model.iter)\n",
    "#         if not model.earlystop_spk:\n",
    "#             optimizers['spk'].step()\n",
    "#             writer.add_scalar('train/loss/spk/spk', loss_spk, model.iter)\n",
    "#             writer.add_scalar('train/loss/spk/gen', loss_gen, model.iter)\n",
    "#             writer.add_scalar('train/loss/spk/age', loss_age, model.iter)\n",
    "\n",
    "        if model.hparams.setting == 'full':\n",
    "            int_acc, int_f1, int_wacc = acc_fn(out_int, y_int)\n",
    "            writer.add_scalar('train/loss/int_loss', loss_int, model.iter)\n",
    "            writer.add_scalar('train/acc/int_acc',int_acc, model.iter)\n",
    "        \n",
    "    elif model.stage == 2:\n",
    "        if model.hparams.setting == 'aux':\n",
    "            out_av, out_at, out_vt = results\n",
    "#             loss_spk = model.hparams.alpha_spk * model.loss(out_spk, spk).mean()\n",
    "#             loss_gen = model.hparams.alpha_gen * model.loss(out_gen, gen).mean()\n",
    "#             loss_age = model.hparams.alpha_age * model.reg_loss(out_age, age).mean()\n",
    "            loss_av = model.loss(out_av, y_int).mean()\n",
    "            loss_at = model.loss(out_at, y_int).mean()\n",
    "            loss_vt = model.loss(out_vt, y_int).mean()\n",
    "            loss = (loss_av + loss_at + loss_vt)/3\n",
    "#             loss += loss_spk + loss_gen + loss_age\n",
    "        elif model.hparams.setting == 'full':\n",
    "            out_int = results\n",
    "#             loss_spk = model.hparams.alpha_spk * model.loss(out_spk, spk).mean()\n",
    "#             loss_gen = model.hparams.alpha_gen * model.loss(out_gen, gen).mean()\n",
    "#             loss_age = model.hparams.alpha_age * model.reg_loss(out_age, age).mean()\n",
    "            loss_int = model.loss(out_int, y_int).mean()\n",
    "            loss = loss_int\n",
    "\n",
    "        optimizers['av'].zero_grad()\n",
    "        optimizers['at'].zero_grad()\n",
    "        optimizers['vt'].zero_grad()\n",
    "#         optimizers['spk'].zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), model.hparams.clip)\n",
    "        if not model.earlystop_av:\n",
    "            optimizers['av'].step()\n",
    "            if model.hparams.setting == 'aux':\n",
    "                av_acc, av_f1, av_wacc = acc_fn(out_av, y_int)\n",
    "                writer.add_scalar('train/stage2/loss/av_loss', loss_av, model.iter)\n",
    "                writer.add_scalar('train/stage2/acc/av_acc', av_acc, model.iter)\n",
    "        if not model.earlystop_at:\n",
    "            optimizers['at'].step()\n",
    "            if model.hparams.setting == 'aux':\n",
    "                at_acc, at_f1, at_wacc = acc_fn(out_at, y_int)\n",
    "                writer.add_scalar('train/stage2/loss/at_loss', loss_at, model.iter)\n",
    "                writer.add_scalar('train/stage2/acc/at_acc', at_acc, model.iter)\n",
    "        if not model.earlystop_vt:\n",
    "            optimizers['vt'].step()\n",
    "            if model.hparams.setting == 'aux':\n",
    "                vt_acc, vt_f1, vt_wacc = acc_fn(out_vt, y_int)\n",
    "                writer.add_scalar('train/stage2/loss/vt_loss', loss_vt, model.iter)\n",
    "                writer.add_scalar('train/stage2/acc/vt_acc', vt_acc, model.iter)\n",
    "        \n",
    "#         if not model.earlystop_spk:\n",
    "#             optimizers['spk'].step()        \n",
    "#             writer.add_scalar('train/loss/spk/spk', loss_spk, model.iter)\n",
    "#             writer.add_scalar('train/loss/spk/gen', loss_gen, model.iter)\n",
    "#             writer.add_scalar('train/loss/spk/age', loss_age, model.iter)\n",
    "        if model.hparams.setting == 'full':\n",
    "            int_acc, int_f1, int_wacc = acc_fn(out_int, y_int)\n",
    "            writer.add_scalar('train/loss/int_loss', loss_int, model.iter)\n",
    "            writer.add_scalar('train/acc/int_acc',int_acc, model.iter)\n",
    "        \n",
    "        \n",
    "    elif model.stage == 3:\n",
    "        out_int = results\n",
    "        \n",
    "        loss_int = model.loss(out_int, y_int).mean()\n",
    "#         loss_spk = model.hparams.alpha_spk * model.loss(out_spk, spk).mean()\n",
    "#         loss_gen = model.hparams.alpha_gen * model.loss(out_gen, gen).mean()\n",
    "#         loss_age = model.hparams.alpha_age * model.reg_loss(out_age, age).mean()\n",
    "        \n",
    "        loss = loss_int\n",
    "        \n",
    "        optimizers['int'].zero_grad()\n",
    "#         optimizers['spk'].zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), model.hparams.clip)\n",
    "        optimizers['int'].step()\n",
    "#         if not model.earlystop_spk:\n",
    "#             optimizers['spk'].step()\n",
    "        int_acc, int_f1, int_wacc = acc_fn(out_int, y_int)\n",
    "        \n",
    "#         writer.add_scalar('train/loss/spk/spk', loss_spk, model.iter)\n",
    "#         writer.add_scalar('train/loss/spk/gen', loss_gen, model.iter)\n",
    "#         writer.add_scalar('train/loss/spk/age', loss_age, model.iter)\n",
    "        \n",
    "        if model.hparams.setting == 'aux':\n",
    "            writer.add_scalar('train/stage3/loss/int_loss', loss_int, model.iter)\n",
    "            writer.add_scalar('train/stage3/acc/int_acc',int_acc, model.iter)\n",
    "        elif model.hparams.setting == 'full':\n",
    "            writer.add_scalar('train/loss/int_loss', loss_int, model.iter)\n",
    "            writer.add_scalar('train/acc/int_acc',int_acc, model.iter)\n",
    "            writer.add_scalar('train/acc/int_f1',int_f1, model.iter)\n",
    "            writer.add_scalar('train/acc/int_wacc',int_wacc, model.iter)\n",
    "\n",
    "    model.iter += 1\n",
    "    \n",
    "def batchify(batch, device):\n",
    "    audios = []\n",
    "    max_len = max([x.shape[0] for x in batch[0]])\n",
    "    for audio in batch[0]:\n",
    "        audio = np.pad(audio, ((0,max_len - audio.shape[0]),(0,0)))\n",
    "        audios.append(audio)\n",
    "    audios = np.stack(audios)\n",
    "\n",
    "    imgs = []\n",
    "    max_len = max([x.shape[0] for x in batch[4]])\n",
    "    for img in batch[4]:\n",
    "        img = np.pad(img, ((0,max_len - img.shape[0]),(0,0)))\n",
    "        imgs.append(img)\n",
    "    imgs = np.stack(imgs)\n",
    "    \n",
    "    texts = []\n",
    "    max_len = max([x.shape[0] for x in batch[2]])\n",
    "    for text in batch[2]:\n",
    "        text = np.pad(text, ((0,max_len - text.shape[0]),(0,0)))\n",
    "        texts.append(text)\n",
    "    texts = np.stack(texts)\n",
    "\n",
    "    audios = torch.tensor(audios).to(device)\n",
    "    audio_lens = torch.tensor(batch[1]).to(device)\n",
    "    \n",
    "    imgs = torch.tensor(imgs).to(device)\n",
    "    img_lens = torch.tensor(batch[5]).to(device)\n",
    "    \n",
    "    texts = torch.tensor(texts).to(device)\n",
    "    text_lens = torch.tensor(batch[3]).to(device)\n",
    "    \n",
    "    bs = audios.shape[0]    \n",
    "    return audios, audio_lens, texts, text_lens, imgs, img_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams():\n",
    "    img_dim = 512\n",
    "    audio_dim = 256\n",
    "    text_dim = 768\n",
    "    \n",
    "    audio_h = 64\n",
    "    img_h = 4\n",
    "    text_h = 32\n",
    "    audio_bidirectional = True\n",
    "    img_bidirectional = True\n",
    "    text_bidirectional = True\n",
    "    \n",
    "    in_drop_img = 0.5\n",
    "    \n",
    "    fc1_av = 128\n",
    "    fc1_at = 128\n",
    "    fc1_vt = 128\n",
    "    \n",
    "    fc2_dim = 512\n",
    "    fc_dropout = 0.5\n",
    "    img_layers = 1\n",
    "    audio_layers = 1\n",
    "    text_layers = 1\n",
    "    img_rnn_dropout = 0\n",
    "    text_rnn_dropout = 0\n",
    "    post_rnn_dropout = 0.5\n",
    "    activation = 'gelu2'\n",
    "    decay_step = 400\n",
    "    decay_mag = 0.1\n",
    "    optim_type = 'nadam'\n",
    "    decay_type = 'none'\n",
    "    max_lr = 0.05\n",
    "    onecycle_epochs = 100\n",
    "    final_div_factor = 1e5\n",
    "    clip = 10\n",
    "    \n",
    "    spk_dim = 10\n",
    "    num_spks = 100\n",
    "    \n",
    "    lr = 1e-3\n",
    "    reg = 1e-6\n",
    "    \n",
    "    audio_lr = lr\n",
    "    audio_reg = reg\n",
    "    audio_optim = optim_type\n",
    "    \n",
    "    img_lr = lr\n",
    "    img_reg = reg\n",
    "    img_optim = optim_type\n",
    "    \n",
    "    text_lr = lr\n",
    "    text_reg = reg\n",
    "    text_optim = optim_type\n",
    "    \n",
    "    spk_lr = lr\n",
    "    spk_reg = reg\n",
    "    spk_optim = optim_type\n",
    "    \n",
    "    av_lr = lr\n",
    "    av_reg = reg\n",
    "    av_optim = optim_type\n",
    "    \n",
    "    at_lr = lr\n",
    "    at_reg = reg\n",
    "    at_optim = optim_type\n",
    "    \n",
    "    vt_lr = lr\n",
    "    vt_reg = reg\n",
    "    vt_optim = optim_type\n",
    "    \n",
    "    int_lr = lr\n",
    "    int_reg = reg\n",
    "    int_optim = optim_type\n",
    "    \n",
    "    init_stage = 3\n",
    "    tune_prev = True\n",
    "    \n",
    "    alpha_spk = 2\n",
    "    alpha_gen = 1\n",
    "    alpha_age = 0.1\n",
    "    \n",
    "    masking = False\n",
    "    mask_rate = 0.1\n",
    "    \n",
    "    setting = 'full'\n",
    "    \n",
    "    audio_segment = 20\n",
    "    audio_step = 20\n",
    "    img_segment = 6\n",
    "    img_step = 200\n",
    "    text_segment = 30\n",
    "    img_step = 10\n",
    "    \n",
    "    update_every = 75\n",
    "    validate_every = 10\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "            \n",
    "def train(**kwargs):\n",
    "    def evaluate():\n",
    "        val_iter = iter(val_dataloader)\n",
    "        model.eval()\n",
    "        val_batch = len(val_iter)\n",
    "        # val_batch = 100\n",
    "        labels, val_preds = [], []\n",
    "        for i in range(val_batch):\n",
    "            batch = next(val_iter, None)\n",
    "            if batch is None:\n",
    "                val_iter = iter(val_dataloader)\n",
    "                batch = next(val_iter, None)\n",
    "        #     ys_int = torch.tensor(batch[5]).to(device)\n",
    "            labels.extend(batch[6])\n",
    "\n",
    "            batch = batchify(batch[:6], device)\n",
    "            max_bs = 256\n",
    "            N = batch[0].shape[0]\n",
    "            all_preds = []\n",
    "            for i in range(0,N, max_bs):\n",
    "                segment = [x[i:i+max_bs] for x in batch]\n",
    "                with torch.no_grad():\n",
    "                    preds = model(segment) \n",
    "                all_preds.append(preds)\n",
    "\n",
    "            if isinstance(all_preds[0], tuple):\n",
    "                preds = [torch.cat([x[i] for x in all_preds], 0) for i in range(len(all_preds[0]))]\n",
    "            else:\n",
    "                preds = torch.cat(all_preds, 0)\n",
    "\n",
    "            if isinstance(preds, tuple) or isinstance(preds, list):\n",
    "                preds = [x.mean(0).view(1,-1) for x in preds]\n",
    "            else:\n",
    "                preds = preds.mean(0).view(1,-1)\n",
    "\n",
    "            preds[preds > 0.5] = 1\n",
    "            preds[preds <= 0.5] = 0\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "        labels = np.array(labels)\n",
    "        val_preds = np.array(val_preds)\n",
    "\n",
    "        from sklearn.metrics import f1_score, accuracy_score\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        from data import emotions\n",
    "        from collections import Counter\n",
    "\n",
    "        f1s, waccs = [], []\n",
    "        for emo in range(labels.shape[1]):\n",
    "            sublabels = labels[:,emo]\n",
    "            subpreds = val_preds[:,emo]\n",
    "            print('---',emotions[emo])\n",
    "            f1 = f1_score(sublabels, subpreds, average='binary')\n",
    "            acc = (sublabels == subpreds).mean()\n",
    "            counts = Counter(sublabels)\n",
    "            n, p = counts[0], counts[1]\n",
    "            tn, fp, fn, tp = confusion_matrix(sublabels, subpreds).ravel()\n",
    "            wacc = (tp * n/p + tn) / (2*n)\n",
    "            print(\"F1:\", f1, \"\\nA:\", acc, \"\\nWA:\", wacc)\n",
    "            print(tp,fp,fn,tn)\n",
    "            f1s.append(f1)\n",
    "            waccs.append(wacc)\n",
    "        print(mean(f1s), mean(waccs))\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True,\n",
    "            collate_fn = collates.collate_fn, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "            collate_fn = collates.val_collate_fn, num_workers=0)\n",
    "    hparams = HParams(epoch_size = len(train_dataloader), **kwargs)\n",
    "    \n",
    "    collates.audio_segment = hparams.audio_segment\n",
    "    collates.audio_step = hparams.audio_step\n",
    "    collates.img_segment = hparams.img_segment\n",
    "    collates.img_step = hparams.img_step\n",
    "    collates.text_segment = hparams.text_segment\n",
    "    collates.text_step = hparams.img_step\n",
    "\n",
    "    H = [hparams.audio_h, hparams.img_h, hparams.text_h, hparams.fc1_av, hparams.fc1_at, hparams.fc1_vt, hparams.fc2_dim]\n",
    "    BI = ['bi' if hparams.audio_bidirectional else 'uni',\n",
    "          'bi' if hparams.img_bidirectional else 'uni',\n",
    "          'bi' if hparams.text_bidirectional else 'uni']\n",
    "    lr = [hparams.audio_lr, hparams.img_lr, hparams.text_lr, hparams.int_lr]\n",
    "    reg = [hparams.audio_reg, hparams.img_reg, hparams.text_reg, hparams.int_reg]\n",
    "    cur_datetime = datetime.now().strftime(\"%m-%d-%H:%M:%S\")\n",
    "    name = \"({} {} {}) lr=[{}]-reg=[{}]-H=[{}]-[{}]-img=[{} {}]-audio=[{} {}]-text=[{} {}]\".\\\n",
    "        format(hparams.setting[0],\n",
    "               hparams.tune_prev,\n",
    "               hparams.init_stage,\n",
    "               \" \".join(map(str,lr)),\n",
    "               \" \".join(map(str,reg)),\n",
    "               \" \".join(map(str,H)), \" \".join(BI),\n",
    "               collates.img_segment, collates.img_step,\n",
    "               collates.audio_segment, collates.audio_step,\n",
    "               collates.text_segment, collates.text_step)\n",
    "\n",
    "    if name:\n",
    "        name = \"%s-%s\"%(cur_datetime,name)\n",
    "    else:\n",
    "        name = cur_datetime\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    ## Model init !!!\n",
    "    model, writer = initiate_model(hparams, name)\n",
    "    optimizers = initiate_optimizers(hparams, model)\n",
    "\n",
    "    train_iter = iter(train_dataloader)\n",
    "    data_counter = 0\n",
    "    if model.hparams.setting == 'aux':\n",
    "        earlystopper_audio = EarlyStopping()\n",
    "        earlystopper_img = EarlyStopping()\n",
    "        earlystopper_text = EarlyStopping()\n",
    "        earlystopper_av = EarlyStopping()\n",
    "        earlystopper_at = EarlyStopping()\n",
    "        earlystopper_vt = EarlyStopping()\n",
    "        earlystopper_int = EarlyStopping()\n",
    "    elif model.hparams.setting == 'full':\n",
    "        earlystopper_stage1 = EarlyStopping()\n",
    "        earlystopper_stage2 = EarlyStopping()\n",
    "        earlystopper_stage3 = EarlyStopping()\n",
    "    earlystopper_spk = EarlyStopping(patience=5)\n",
    "\n",
    "    print(\"[Starting]\", model.name)\n",
    "    while True:\n",
    "        \n",
    "        if model.iter % (hparams.update_every*hparams.epoch_size//1) == 0:\n",
    "            train_dataset.update_data()\n",
    "\n",
    "        model.train()\n",
    "        batch = next(train_iter, None)\n",
    "        if batch is None:\n",
    "            train_iter = iter(train_dataloader)\n",
    "            batch = next(train_iter, None)\n",
    "\n",
    "\n",
    "    #     batch = list(batch)\n",
    "    #     batch[0] += np.random.normal(0,0.1,batch[0].shape)\n",
    "    #     batch[2] += np.random.normal(0,0.1,batch[2].shape)\n",
    "    #     batch[3] += np.random.normal(0,0.1,batch[3].shape)\n",
    "\n",
    "\n",
    "        batch = [torch.tensor(x).to(device) for x in batch]\n",
    "        res = train_step(model, optimizers, batch, writer)\n",
    "        if model.iter % (hparams.validate_every*hparams.epoch_size//1) == 0:\n",
    "            val_iter = iter(val_dataloader)\n",
    "            model.eval()\n",
    "            val_batch = 128\n",
    "            results = []\n",
    "            batch_preds = []\n",
    "            batch_ys = []\n",
    "            for i in range(val_batch):\n",
    "                batch = next(val_iter, None)\n",
    "                if batch is None:\n",
    "                    val_iter = iter(val_dataloader)\n",
    "                    batch = next(val_iter, None)\n",
    "                ys_int = torch.tensor(batch[6]).to(device)\n",
    "\n",
    "                batch = batchify(batch[:6], device)\n",
    "                max_bs = 256\n",
    "                N = batch[0].shape[0]\n",
    "                all_preds = []\n",
    "                for i in range(0,N, max_bs):\n",
    "                    segment = [x[i:i+max_bs] for x in batch]\n",
    "                    with torch.no_grad():\n",
    "                        preds = model(segment) \n",
    "                    all_preds.append(preds)\n",
    "\n",
    "                if isinstance(all_preds[0], tuple):\n",
    "                    preds = [torch.cat([x[i] for x in all_preds], 0) for i in range(len(all_preds[0]))]\n",
    "                else:\n",
    "                    preds = torch.cat(all_preds, 0)\n",
    "\n",
    "                if isinstance(preds, tuple) or isinstance(preds, list):\n",
    "                    preds = [x.mean(0).view(1,-1) for x in preds]\n",
    "                else:\n",
    "                    preds = preds.mean(0).view(1,-1)\n",
    "                batch_preds.append(preds)\n",
    "                batch_ys.append(ys_int)\n",
    "\n",
    "            if isinstance(batch_preds[0], tuple) or isinstance(batch_preds[0], list):\n",
    "                preds = [torch.cat(x,0) for x in zip(*batch_preds)]\n",
    "            else:\n",
    "                preds = torch.cat(batch_preds,0)\n",
    "            ys_int = torch.cat(batch_ys,0)\n",
    "\n",
    "            if model.hparams.setting == 'aux' and (model.stage == 1 or model.stage == 2):\n",
    "                if model.stage == 1:\n",
    "                    with torch.no_grad():\n",
    "                        loss_audio = model.loss(preds[0], ys_int).mean().item()\n",
    "                        loss_img = model.loss(preds[1], ys_int).mean().item()\n",
    "                        loss_text = model.loss(preds[2], ys_int).mean().item()\n",
    "    #                         loss_spk = model.hparams.alpha_spk * model.loss(preds[3], spk).mean()\n",
    "    #                         loss_gen = model.hparams.alpha_gen * model.loss(preds[4], gen).mean()\n",
    "    #                         loss_age = model.hparams.alpha_age * model.reg_loss(preds[5], age).mean()\n",
    "                    audio_acc, audio_f1, audio_wacc = acc_fn(preds[0], ys_int)\n",
    "                    img_acc, img_f1, img_wacc = acc_fn(preds[1], ys_int)\n",
    "                    text_acc, test_f1, text_wacc = acc_fn(preds[2], ys_int)\n",
    "\n",
    "                    results.append((loss_audio, loss_img, loss_text,\n",
    "                                   audio_acc, img_acc, text_acc,\n",
    "    #                                    loss_spk, loss_gen, loss_age\n",
    "                                   ))\n",
    "                elif model.stage == 2:\n",
    "                    with torch.no_grad():\n",
    "                        loss_av = model.loss(preds[0], ys_int).mean().item()\n",
    "                        loss_at = model.loss(preds[1], ys_int).mean().item()\n",
    "                        loss_vt = model.loss(preds[2], ys_int).mean().item()\n",
    "    #                         loss_spk = model.hparams.alpha_spk * model.loss(preds[3], spk).mean()\n",
    "    #                         loss_gen = model.hparams.alpha_gen * model.loss(preds[4], gen).mean()\n",
    "    #                         loss_age = model.hparams.alpha_age * model.reg_loss(preds[5], age).mean()\n",
    "\n",
    "                    av_acc, av_f1, av_wacc = acc_fn(preds[0], ys_int)\n",
    "                    at_acc, at_f1, at_wacc = acc_fn(preds[1], ys_int)\n",
    "                    vt_acc, vt_f1, av_wacc = acc_fn(preds[2], ys_int)\n",
    "\n",
    "                    results.append((loss_av, loss_at, loss_vt,\n",
    "                                   av_acc, at_acc, vt_acc,\n",
    "    #                                    loss_spk, loss_gen, loss_age\n",
    "                                   ))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    loss_int = model.loss(preds, ys_int).mean().item()\n",
    "    #                     loss_spk = model.hparams.alpha_spk * model.loss(preds[1], spk).mean()\n",
    "    #                     loss_gen = model.hparams.alpha_gen * model.loss(preds[2], gen).mean()\n",
    "    #                     loss_age = model.hparams.alpha_age * model.reg_loss(preds[3], age).mean()\n",
    "\n",
    "                int_acc, int_f1, int_wacc = acc_fn(preds, ys_int)\n",
    "                results.append((loss_int, int_acc, int_f1, int_wacc\n",
    "    #                                 loss_spk, loss_gen, loss_age\n",
    "                               ))\n",
    "\n",
    "            results = [mean([res[i] for res in results]) for i in range(len(results[0]))]\n",
    "\n",
    "            if model.hparams.setting == 'aux':\n",
    "                if model.stage == 1:\n",
    "                    loss_audio, loss_img, loss_text, audio_acc, img_acc, text_acc = results\n",
    "\n",
    "                    if not model.earlystop_audio and earlystopper_audio(loss_audio):\n",
    "                        model.earlystop_audio = True\n",
    "                        print(\"[%d] Stopped audio\"%model.iter)\n",
    "                    if not model.earlystop_img and earlystopper_img(loss_img):\n",
    "                        model.earlystop_img = True\n",
    "                        print(\"[%d] Stopped img\"%model.iter)\n",
    "                    if not model.earlystop_text and earlystopper_text(loss_text):\n",
    "                        model.earlystop_text = True\n",
    "                        print(\"[%d] Stopped text\"%model.iter)\n",
    "    #                 if earlystopper_spk(loss_spk):\n",
    "    #                     model.earlystop_spk = True\n",
    "\n",
    "                    if not model.earlystop_audio:\n",
    "                        writer.add_scalar('val/stage1/loss/audio_loss', loss_audio, model.iter)\n",
    "                        writer.add_scalar('val/stage1/acc/audio_acc',audio_acc, model.iter)\n",
    "                    if not model.earlystop_img:\n",
    "                        writer.add_scalar('val/stage1/loss/img_loss', loss_img, model.iter)\n",
    "                        writer.add_scalar('val/stage1/acc/img_acc',img_acc, model.iter)\n",
    "                    if not model.earlystop_text:\n",
    "                        writer.add_scalar('val/stage1/loss/text_loss', loss_text, model.iter)\n",
    "                        writer.add_scalar('val/stage1/acc/text_acc',text_acc, model.iter)\n",
    "\n",
    "                    if model.earlystop_audio and model.earlystop_img and model.earlystop_text:\n",
    "                        model.stage = 2\n",
    "                        model.earlystop_spk = False\n",
    "                        earlystopper_spk.counter = 0\n",
    "                elif model.stage == 2:\n",
    "                    loss_av, loss_at, loss_vt, av_acc, at_acc, vt_acc = results\n",
    "\n",
    "                    if not model.earlystop_av and earlystopper_av(loss_av):\n",
    "                        model.earlystop_av = True\n",
    "                        print(\"[%d] Stopped av\"%model.iter)\n",
    "                    if not model.earlystop_at and earlystopper_at(loss_at):\n",
    "                        model.earlystop_at = True\n",
    "                        print(\"[%d] Stopped at\"%model.iter)\n",
    "                    if not model.earlystop_vt and earlystopper_vt(loss_vt):\n",
    "                        model.earlystop_vt = True\n",
    "                        print(\"[%d] Stopped vt\"%model.iter)\n",
    "    #                 if earlystopper_spk(loss_spk):\n",
    "    #                     model.earlystop_spk = True\n",
    "\n",
    "                    if not model.earlystop_av:\n",
    "                        writer.add_scalar('val/stage2/loss/av_loss', loss_av, model.iter)\n",
    "                        writer.add_scalar('val/stage2/acc/av_acc',av_acc, model.iter)\n",
    "                    if not model.earlystop_at:\n",
    "                        writer.add_scalar('val/stage2/loss/at_loss', loss_at, model.iter)\n",
    "                        writer.add_scalar('val/stage2/acc/at_acc',at_acc, model.iter)\n",
    "                    if not model.earlystop_vt:\n",
    "                        writer.add_scalar('val/stage2/loss/vt_loss', loss_vt, model.iter)\n",
    "                        writer.add_scalar('val/stage2/acc/vt_acc',vt_acc, model.iter)\n",
    "\n",
    "                    if model.earlystop_av and model.earlystop_at and model.earlystop_vt:\n",
    "                        model.stage = 3\n",
    "                elif model.stage == 3:\n",
    "                    loss_int, int_acc, int_f1, int_wacc = results\n",
    "\n",
    "                    writer.add_scalar('val/stage3/loss/int_loss', loss_int, model.iter)\n",
    "                    writer.add_scalar('val/stage3/acc/int_acc',int_acc, model.iter)\n",
    "                    writer.add_scalar('val/stage3/acc/int_f1', int_f1, model.iter)\n",
    "                    writer.add_scalar('val/stage3/acc/int_wacc',int_wacc, model.iter)\n",
    "\n",
    "\n",
    "                    if earlystopper_int(loss_int):\n",
    "                        print(\"[%d] Stopped int\"%model.iter)\n",
    "    #                     train_dataset.update_data()\n",
    "    #                     model.reset_state()\n",
    "    #                     earlystopper_audio = EarlyStopping()\n",
    "    #                     earlystopper_img = EarlyStopping()\n",
    "    #                     earlystopper_text = EarlyStopping()\n",
    "    #                     earlystopper_av = EarlyStopping()\n",
    "    #                     earlystopper_at = EarlyStopping()\n",
    "    #                     earlystopper_vt = EarlyStopping()\n",
    "    #                     earlystopper_int = EarlyStopping()\n",
    "                        break\n",
    "\n",
    "            elif model.hparams.setting == 'full':\n",
    "                loss_int, int_acc, int_f1, int_wacc = results\n",
    "                if model.stage == 1:\n",
    "    #                 if earlystopper_spk(loss_spk):\n",
    "    #                     model.earlystop_spk = True\n",
    "                    if earlystopper_stage1(loss_int):\n",
    "                        model.stage = 2\n",
    "                        print(\"[%d] Finished stage 1\"%model.iter)\n",
    "                        model.earlystop_spk = False\n",
    "                        earlystopper_spk.counter = 0\n",
    "                elif model.stage == 2:\n",
    "    #                 if earlystopper_spk(loss_spk):\n",
    "    #                     model.earlystop_spk = True\n",
    "                    if earlystopper_stage2(loss_int):\n",
    "                        model.stage = 3\n",
    "                        print(\"[%d] Finished stage 2\"%model.iter)\n",
    "                elif model.stage == 3:\n",
    "    #                 if earlystopper_spk(loss_spk):\n",
    "    #                     model.earlystop_spk = True\n",
    "                    if earlystopper_stage3(loss_int):\n",
    "                        print(\"[%d] Finished stage 3\"%model.iter)\n",
    "    #                     train_dataset.update_data()\n",
    "    #                     model.reset_state()\n",
    "    #                     earlystopper_audio = EarlyStopping()\n",
    "    #                     earlystopper_img = EarlyStopping()\n",
    "    #                     earlystopper_text = EarlyStopping()\n",
    "    #                     earlystopper_av = EarlyStopping()\n",
    "    #                     earlystopper_at = EarlyStopping()\n",
    "    #                     earlystopper_vt = EarlyStopping()\n",
    "    #                     earlystopper_int = EarlyStopping()\n",
    "                        break\n",
    "                writer.add_scalar('val/loss/int_loss', loss_int, model.iter)\n",
    "                writer.add_scalar('val/acc/int_acc',int_acc, model.iter)\n",
    "                writer.add_scalar('val/acc/int_f1', int_f1, model.iter)\n",
    "                writer.add_scalar('val/acc/int_wacc',int_wacc, model.iter)\n",
    "\n",
    "    #         if not model.earlystop_spk:\n",
    "    #             writer.add_scalar('val/loss/spk/spk', loss_spk, model.iter)\n",
    "    #             writer.add_scalar('val/loss/spk/gen', loss_gen, model.iter)\n",
    "    #             writer.add_scalar('val/loss/spk/age', loss_age, model.iter)\n",
    "\n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-00:17:00-(f True 3) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[6 10]-audio=[20 20]-text=[30 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:51<00:00,  2.91it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:54<00:00,  2.86it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:53<00:00,  2.88it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:53<00:00,  2.89it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.04it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.94it/s]\n",
      "100%|██████████| 500/500 [02:51<00:00,  2.91it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.02it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.02it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.94it/s]\n",
      "100%|██████████| 500/500 [02:37<00:00,  3.17it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:58<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15680] Finished stage 3\n",
      "--- hap\n",
      "F1: 0.9081455805892548 \n",
      "A: 0.8398791540785498 \n",
      "WA: 0.6221664178091488\n",
      "262 37 16 16\n",
      "--- sad\n",
      "F1: 0.6458923512747875 \n",
      "A: 0.622356495468278 \n",
      "WA: 0.6205798374698002\n",
      "114 65 60 92\n",
      "--- fea\n",
      "F1: 0.40404040404040403 \n",
      "A: 0.4652567975830816 \n",
      "WA: 0.5227396761870111\n",
      "60 145 32 94\n",
      "--- sur\n",
      "F1: 0.5691056910569106 \n",
      "A: 0.6797583081570997 \n",
      "WA: 0.681565202500214\n",
      "70 74 32 155\n",
      "--- ang\n",
      "F1: 0.4467005076142132 \n",
      "A: 0.6706948640483383 \n",
      "WA: 0.6027149321266968\n",
      "44 43 66 178\n",
      "--- dis\n",
      "F1: 0.5533980582524272 \n",
      "A: 0.7220543806646526 \n",
      "WA: 0.6778303917348256\n",
      "57 48 44 182\n",
      "0.5878804321379995 0.6212660763046162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-01:50:50-(f False 1) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[6 10]-audio=[20 20]-text=[30 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:50<00:00,  2.94it/s]\n",
      "100%|██████████| 500/500 [02:34<00:00,  3.23it/s]\n",
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.97it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n",
      "100%|██████████| 500/500 [02:39<00:00,  3.13it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:37<00:00,  3.18it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.00it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.94it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.93it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n",
      "100%|██████████| 500/500 [02:51<00:00,  2.92it/s]\n",
      "100%|██████████| 500/500 [02:37<00:00,  3.18it/s]\n",
      "100%|██████████| 500/500 [02:34<00:00,  3.23it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.12it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.00it/s]\n",
      "100%|██████████| 500/500 [02:53<00:00,  2.88it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:31<00:00,  3.29it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.12it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:51<00:00,  2.92it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.96it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.11it/s]\n",
      "100%|██████████| 500/500 [02:55<00:00,  2.84it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22680] Finished stage 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:39<00:00,  3.14it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.97it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:39<00:00,  3.14it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.93it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:35<00:00,  3.22it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.96it/s]\n",
      "100%|██████████| 500/500 [02:37<00:00,  3.18it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29120] Finished stage 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:39<00:00,  3.13it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.02it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:52<00:00,  2.91it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:37<00:00,  3.17it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.12it/s]\n",
      "100%|██████████| 500/500 [02:39<00:00,  3.13it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.09it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:53<00:00,  2.89it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.00it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.09it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:37<00:00,  3.18it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.94it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:51<00:00,  2.91it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.11it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.93it/s]\n",
      "100%|██████████| 500/500 [02:36<00:00,  3.19it/s]\n",
      "100%|██████████| 500/500 [02:51<00:00,  2.91it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.11it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.96it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.04it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.09it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.94it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.96it/s]\n",
      "100%|██████████| 500/500 [02:35<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57120] Finished stage 3\n",
      "--- hap\n",
      "F1: 0.9093904448105437 \n",
      "A: 0.8338368580060423 \n",
      "WA: 0.49640287769784175\n",
      "276 53 2 0\n",
      "--- sad\n",
      "F1: 0.6773455377574371 \n",
      "A: 0.5740181268882175 \n",
      "WA: 0.5590453181052786\n",
      "148 115 26 42\n",
      "--- fea\n",
      "F1: 0.41322314049586784 \n",
      "A: 0.3564954682779456 \n",
      "WA: 0.4975668546479898\n",
      "75 196 17 43\n",
      "--- sur\n",
      "F1: 0.5467128027681661 \n",
      "A: 0.6042296072507553 \n",
      "WA: 0.6514470416987757\n",
      "79 108 23 121\n",
      "--- ang\n",
      "F1: 0.38738738738738737 \n",
      "A: 0.5891238670694864 \n",
      "WA: 0.5393459481694776\n",
      "43 69 67 152\n",
      "--- dis\n",
      "F1: 0.5546218487394958 \n",
      "A: 0.6797583081570997 \n",
      "WA: 0.6723848471803702\n",
      "66 71 35 159\n",
      "0.581446860326483 0.5693654812499557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-07:18:11-(a False 1) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[6 10]-audio=[20 20]-text=[30 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 242/500 [01:14<01:19,  3.25it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5df44edd4c6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'aux'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_stage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m train(setting='aux', init_stage = 1, tune_prev = False,\n\u001b[1;32m    451\u001b[0m      \u001b[0maudio_segment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-5df44edd4c6c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/multimodal-emo-rec/data.py\u001b[0m in \u001b[0;36mupdate_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s_long.npz\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'word_embed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s_pase.npy\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s_img.npz\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/env/lib/python3.6/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(setting='full', init_stage = 3, tune_prev = True)\n",
    "train(setting='full', init_stage = 1, tune_prev = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-13:47:05-(a False 1) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[6 10]-audio=[20 20]-text=[30 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.00it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.93it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2320] Stopped img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:45<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:35<00:00,  3.21it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4240] Stopped text\n",
      "[4400] Stopped audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6160] Stopped av\n",
      "[6160] Stopped vt\n",
      "[6320] Stopped at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:53<00:00,  2.89it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n",
      "100%|██████████| 500/500 [02:53<00:00,  2.89it/s]\n",
      "100%|██████████| 500/500 [02:39<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9440] Stopped int\n",
      "--- hap\n",
      "F1: 0.8975265017667845 \n",
      "A: 0.824773413897281 \n",
      "WA: 0.6360798153929685\n",
      "254 34 24 19\n",
      "--- sad\n",
      "F1: 0.6666666666666667 \n",
      "A: 0.6163141993957704 \n",
      "WA: 0.6101654586719378\n",
      "127 80 47 77\n",
      "--- fea\n",
      "F1: 0.42585551330798477 \n",
      "A: 0.5438066465256798 \n",
      "WA: 0.5637620520283791\n",
      "56 115 36 124\n",
      "--- sur\n",
      "F1: 0.5714285714285713 \n",
      "A: 0.6737160120845922 \n",
      "WA: 0.6826354996146929\n",
      "72 78 30 151\n",
      "--- ang\n",
      "F1: 0.4585365853658537 \n",
      "A: 0.6646525679758308 \n",
      "WA: 0.6050390785684904\n",
      "47 48 63 173\n",
      "--- dis\n",
      "F1: 0.5538461538461539 \n",
      "A: 0.7371601208459214 \n",
      "WA: 0.6803702109341369\n",
      "54 40 47 190\n",
      "0.595643332063669 0.629675352535101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-14:47:39-(a True 1) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[6 10]-audio=[20 20]-text=[30 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.09it/s]\n",
      "100%|██████████| 500/500 [02:37<00:00,  3.17it/s]\n",
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000] Stopped text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:40<00:00,  3.12it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.00it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7600] Stopped audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8480] Stopped img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:38<00:00,  3.16it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11760] Stopped av\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:50<00:00,  2.93it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.10it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.02it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.04it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14960] Stopped at\n",
      "[14960] Stopped vt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:44<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:51<00:00,  2.92it/s]\n",
      "100%|██████████| 500/500 [02:40<00:00,  3.12it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19280] Stopped int\n",
      "--- hap\n",
      "F1: 0.8876811594202898 \n",
      "A: 0.8126888217522659 \n",
      "WA: 0.6670625763540111\n",
      "245 29 33 24\n",
      "--- sad\n",
      "F1: 0.6297376093294461 \n",
      "A: 0.6163141993957704 \n",
      "WA: 0.61607731166264\n",
      "108 61 66 96\n",
      "--- fea\n",
      "F1: 0.3835616438356165 \n",
      "A: 0.4561933534743202 \n",
      "WA: 0.5030925959614335\n",
      "56 144 36 95\n",
      "--- sur\n",
      "F1: 0.4973544973544973 \n",
      "A: 0.7129909365558912 \n",
      "WA: 0.6430559123212604\n",
      "47 40 55 189\n",
      "--- ang\n",
      "F1: 0.425531914893617 \n",
      "A: 0.6737160120845922 \n",
      "WA: 0.5958453311394488\n",
      "40 38 70 183\n",
      "--- dis\n",
      "F1: 0.5319148936170213 \n",
      "A: 0.7341389728096677 \n",
      "WA: 0.6670899698665519\n",
      "50 37 51 193\n",
      "0.5592969530750814 0.6153706162175576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-16:52:09-(a False 1) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[16 1]-audio=[20 1]-text=[30 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 7.93 GiB total capacity; 2.45 GiB already allocated; 2.64 GiB free; 2.29 GiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8ff7d98e08f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    457\u001b[0m      \u001b[0maudio_segment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m      \u001b[0mimg_segment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m      text_segment=30, text_step=1)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-8ff7d98e08f2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mys_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m                 \u001b[0mmax_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-8d3cda04a51c>\u001b[0m in \u001b[0;36mbatchify\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mimg_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0mtext_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.32 GiB (GPU 0; 7.93 GiB total capacity; 2.45 GiB already allocated; 2.64 GiB free; 2.29 GiB cached)"
     ]
    }
   ],
   "source": [
    "train(setting='aux', init_stage = 1, tune_prev = False)\n",
    "train(setting='aux', init_stage = 1, tune_prev = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-19:53:56-(a True 3) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[6 10]-audio=[20 20]-text=[30 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:53<00:00,  2.88it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.02it/s]\n",
      "100%|██████████| 500/500 [02:41<00:00,  3.09it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.97it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3280] Stopped int\n",
      "--- hap\n",
      "F1: 0.8700173310225303 \n",
      "A: 0.7734138972809668 \n",
      "WA: 0.49860866024161804\n",
      "251 48 27 5\n",
      "--- sad\n",
      "F1: 0.562300319488818 \n",
      "A: 0.5861027190332326 \n",
      "WA: 0.5904531810527857\n",
      "88 51 86 106\n",
      "--- fea\n",
      "F1: 0.36363636363636365 \n",
      "A: 0.5770392749244713 \n",
      "WA: 0.5332908859377843\n",
      "40 88 52 151\n",
      "--- sur\n",
      "F1: 0.37575757575757573 \n",
      "A: 0.6888217522658611 \n",
      "WA: 0.5820917886805378\n",
      "31 32 71 197\n",
      "--- ang\n",
      "F1: 0.2967741935483871 \n",
      "A: 0.6706948640483383 \n",
      "WA: 0.554771698889346\n",
      "23 22 87 199\n",
      "--- dis\n",
      "F1: 0.4268292682926829 \n",
      "A: 0.716012084592145 \n",
      "WA: 0.612397761515282\n",
      "35 28 66 202\n",
      "0.4825525086243929 0.561935662719559\n"
     ]
    }
   ],
   "source": [
    "train(setting='aux', init_stage = 3, tune_prev = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-18:35:26-(a False 1) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[16 1]-audio=[20 1]-text=[30 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [03:11<00:00,  2.61it/s]\n",
      "100%|██████████| 500/500 [02:57<00:00,  2.82it/s]\n",
      "100%|██████████| 500/500 [02:54<00:00,  2.87it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2320] Stopped audio\n",
      "[2320] Stopped img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [03:02<00:00,  2.74it/s]\n",
      "100%|██████████| 500/500 [02:48<00:00,  2.97it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4960] Stopped text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:56<00:00,  2.83it/s]\n",
      "100%|██████████| 500/500 [02:57<00:00,  2.82it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.04it/s]\n",
      "100%|██████████| 500/500 [02:55<00:00,  2.85it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9520] Stopped at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9600] Stopped av\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:44<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:43<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10400] Stopped vt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:54<00:00,  2.87it/s]\n",
      "100%|██████████| 500/500 [02:42<00:00,  3.08it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:39<00:00,  3.13it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.98it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14160] Stopped int\n",
      "--- hap\n",
      "F1: 0.8782287822878229 \n",
      "A: 0.8006042296072508 \n",
      "WA: 0.6827745350889101\n",
      "238 26 40 27\n",
      "--- sad\n",
      "F1: 0.6855670103092784 \n",
      "A: 0.6314199395770392 \n",
      "WA: 0.6242221246064866\n",
      "133 81 41 76\n",
      "--- fea\n",
      "F1: 0.4565217391304348 \n",
      "A: 0.5468277945619335 \n",
      "WA: 0.5892532290340186\n",
      "63 121 29 118\n",
      "--- sur\n",
      "F1: 0.5502183406113538 \n",
      "A: 0.6888217522658611 \n",
      "WA: 0.6690855381453892\n",
      "63 64 39 165\n",
      "--- ang\n",
      "F1: 0.4818181818181818 \n",
      "A: 0.6555891238670695 \n",
      "WA: 0.6119498148909914\n",
      "53 57 57 164\n",
      "--- dis\n",
      "F1: 0.5844748858447489 \n",
      "A: 0.7250755287009063 \n",
      "WA: 0.6994403788204908\n",
      "64 54 37 176\n",
      "0.6061381566669701 0.6461209367643811\n"
     ]
    }
   ],
   "source": [
    "train(setting='aux', init_stage = 1, tune_prev = False,\n",
    "     audio_segment=20, audio_step=1,\n",
    "     img_segment=16, img_step=1,\n",
    "     text_segment=30, text_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Starting] 12-06-20:46:43-(a False 1) lr=[0.001 0.001 0.001 0.001]-reg=[1e-06 1e-06 1e-06 1e-06]-H=[64 4 32 128 128 128 512]-[bi bi bi]-img=[6 1]-audio=[20 1]-text=[30 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:49<00:00,  2.96it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.93it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2400] Stopped img\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:42<00:00,  3.07it/s]\n",
      "100%|██████████| 500/500 [02:38<00:00,  3.15it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.00it/s]\n",
      "100%|██████████| 500/500 [02:49<00:00,  2.95it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4880] Stopped audio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:47<00:00,  2.99it/s]\n",
      "100%|██████████| 500/500 [02:39<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6400] Stopped text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:45<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:50<00:00,  2.93it/s]\n",
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8960] Stopped at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:44<00:00,  3.05it/s]\n",
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000] Stopped av\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:44<00:00,  3.03it/s]\n",
      "100%|██████████| 500/500 [02:45<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11200] Stopped vt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:48<00:00,  2.97it/s]\n",
      "100%|██████████| 500/500 [02:46<00:00,  3.01it/s]\n",
      "100%|██████████| 500/500 [02:52<00:00,  2.91it/s]\n",
      "100%|██████████| 500/500 [02:36<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13680] Stopped int\n",
      "--- hap\n",
      "F1: 0.8884826325411335 \n",
      "A: 0.8157099697885196 \n",
      "WA: 0.6917673408443058\n",
      "243 26 35 27\n",
      "--- sad\n",
      "F1: 0.6738544474393532 \n",
      "A: 0.6344410876132931 \n",
      "WA: 0.6298960392415257\n",
      "125 72 49 85\n",
      "--- fea\n",
      "F1: 0.39416058394160586 \n",
      "A: 0.4984894259818731 \n",
      "WA: 0.5256958340913226\n",
      "54 128 38 111\n",
      "--- sur\n",
      "F1: 0.5418719211822661 \n",
      "A: 0.7190332326283988 \n",
      "WA: 0.6691711619145475\n",
      "55 46 47 183\n",
      "--- ang\n",
      "F1: 0.4153005464480874 \n",
      "A: 0.676737160120846 \n",
      "WA: 0.5935417523652818\n",
      "38 35 72 186\n",
      "--- dis\n",
      "F1: 0.4971751412429379 \n",
      "A: 0.7311178247734139 \n",
      "WA: 0.6482565647869135\n",
      "44 32 57 198\n",
      "0.568474212132564 0.6263881155406495\n"
     ]
    }
   ],
   "source": [
    "train(setting='aux', init_stage = 1, tune_prev = False,\n",
    "     audio_segment=20, audio_step=1,\n",
    "     img_segment=6, img_step=1,\n",
    "     text_segment=30, text_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [02:35<00:00,  2.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset.update_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collates():\n",
    "    audio_segment = 30\n",
    "    audio_step = 1\n",
    "    img_segment = 30\n",
    "    text_segment = 30\n",
    "    text_step = 1\n",
    "    img_step = 1\n",
    "    fixed_size = False\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "      audios, texts, imgs, ys_int = [list(x) for x in zip(*batch)]\n",
    "    \n",
    "#       audio_mean = np.array([audio.mean(0) for audio in audios])\n",
    "#       img_mean = np.array([img.mean(0) for img in imgs])\n",
    "    \n",
    "      audio_len = []\n",
    "      for i, audio in enumerate(audios):\n",
    "#         start_id = np.random.randint(0,max(1,audio.shape[0] - self.audio_step*self.audio_segment + 1))\n",
    "        start_id = np.random.randint(0,audio.shape[0])\n",
    "        audios[i] = audio[start_id:start_id + self.audio_step*self.audio_segment:self.audio_step]\n",
    "        \n",
    "      for i, img in enumerate(imgs):\n",
    "#         start_id = np.random.randint(0, max(img.shape[0] - self.img_step*self.img_segment + 1, 1))\n",
    "        start_id = np.random.randint(0, img.shape[0])\n",
    "        imgs[i] = img[start_id:start_id + self.img_step*self.img_segment:self.img_step]\n",
    "        \n",
    "      for i, text in enumerate(texts):\n",
    "#         start_id = np.random.randint(0, max(img.shape[0] - self.img_step*self.img_segment + 1, 1))\n",
    "        start_id = np.random.randint(0, text.shape[0])\n",
    "        texts[i] = text[start_id:start_id + self.text_step*self.text_segment:self.text_step]\n",
    "\n",
    "#       text_lens = [text.shape[0] for text in texts]\n",
    "#       max_text_len = max(text_lens)\n",
    "#       for i,text in enumerate(texts):\n",
    "#         n = text_lens[i]\n",
    "#         texts[i] = np.pad(text, ((0,max_text_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "      img_lens = [img.shape[0] for img in imgs]\n",
    "      if self.fixed_size:\n",
    "        max_img_len = self.img_segment\n",
    "      else:\n",
    "        max_img_len = max(img_lens)\n",
    "      for i,img in enumerate(imgs):\n",
    "        n = img_lens[i]\n",
    "        imgs[i] = np.pad(img, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "      audio_lens = [audio.shape[0] for audio in audios]\n",
    "      if self.fixed_size:\n",
    "        max_audio_len = self.audio_segment\n",
    "      else:\n",
    "        max_audio_len = max(audio_lens)\n",
    "      for i,audio in enumerate(audios):\n",
    "        n = audio_lens[i]\n",
    "        audios[i] = np.pad(audio, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "        \n",
    "      text_lens = [text.shape[0] for text in texts]\n",
    "      if self.fixed_size:\n",
    "        max_text_len = self.text_segment\n",
    "      else:\n",
    "        max_text_len = max(text_lens)\n",
    "      for i,text in enumerate(texts):\n",
    "        n = text_lens[i]\n",
    "        texts[i] = np.pad(text, ((0,max_text_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        \n",
    "#       audio_mean = np.repeat(np.expand_dims(audio_mean, 1), max_audio_len, 1)\n",
    "#       img_mean = np.repeat(np.expand_dims(img_mean, 1), max_img_len, 1)\n",
    "        \n",
    "      audios = np.stack(audios)\n",
    "#       audios = np.concatenate([audios, audio_mean], -1)\n",
    "      imgs = np.stack(imgs)\n",
    "#       imgs = np.concatenate([imgs, img_mean], -1)\n",
    "      texts = np.stack(texts)\n",
    "\n",
    "        \n",
    "      res = audios, audio_lens, \\\n",
    "        texts, text_lens, \\\n",
    "        imgs, img_lens, np.array(ys_int).astype('float32')\n",
    "      return res\n",
    "\n",
    "    def val_collate_fn(self, batch):\n",
    "      entry = batch[0]\n",
    "      audio = entry[0]\n",
    "      text = entry[1]\n",
    "      img = entry[2]\n",
    "      if len(entry) == 4:\n",
    "        ys_int = np.array([entry[3]]).astype('float32')\n",
    "      else:\n",
    "        ys_int = None\n",
    "\n",
    "      audios, imgs, texts = [], [], []\n",
    "      img_lens, audio_lens, text_lens = [], [], []\n",
    "      audio_n = audio.shape[0]\n",
    "      img_n = img.shape[0]\n",
    "      text_n = text.shape[0]\n",
    "        \n",
    "#       audio_mean = audio.mean(0)\n",
    "#       img_mean = img.mean(0)\n",
    "\n",
    "      if self.audio_step > 1:\n",
    "        audio_off_step = self.audio_step\n",
    "      else:\n",
    "        audio_off_step = audio_n//10\n",
    "      if self.img_step > 1:\n",
    "        img_off_step = self.img_step\n",
    "      else:\n",
    "        img_off_step = img_n//10\n",
    "      if self.text_step > 1:\n",
    "        text_off_step = self.text_step\n",
    "      else:\n",
    "        text_off_step = text_n//10\n",
    "\n",
    "      i = 0\n",
    "      while True:\n",
    "        audio_offset = i*audio_off_step\n",
    "        img_offset = i*img_off_step\n",
    "        text_offset = i*text_off_step\n",
    "\n",
    "        \n",
    "        if audio_offset >= audio_n and img_offset >= img_n and text_offset >= text_n:\n",
    "            break\n",
    "\n",
    "        if audio_offset < audio_n:\n",
    "          audio_segment = audio[audio_offset:audio_offset + self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(1,audio_n - self.audio_step*self.audio_segment + 1))\n",
    "          start_id = np.random.randint(0, audio_n)\n",
    "          audio_segment = audio[start_id:start_id+self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "\n",
    "        if img_offset < img_n:\n",
    "            img_segment = img[img_offset:img_offset + self.img_step*self.img_segment:self.img_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(img_n - self.img_step*self.img_segment + 1, 1))\n",
    "          start_id = np.random.randint(0, img_n)\n",
    "          img_segment = img[start_id:start_id+self.img_step*self.img_segment:self.img_step, :]\n",
    "        \n",
    "        if text_offset < text_n:\n",
    "            text_segment = text[text_offset:text_offset + self.text_step*self.text_segment:self.text_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(text_n - self.text_step*self.text_segment + 1, 1))\n",
    "          start_id = np.random.randint(0, text_n)\n",
    "          text_segment = text[start_id:start_id+self.text_step*self.text_segment:self.text_step, :]\n",
    "        \n",
    "#         audio_segment = np.concatenate([audio, np.repeat(np.expand_dims(audio_mean, 0), audio.shape[0], 0)], -1)\n",
    "#         img_segment = np.concatenate([img, np.repeat(np.expand_dims(img_mean, 0), img.shape[0], 0)], -1)\n",
    "    \n",
    "        img_lens.append(img_segment.shape[0])\n",
    "        audio_lens.append(audio_segment.shape[0])\n",
    "        text_lens.append(text_segment.shape[0])\n",
    "        \n",
    "        \n",
    "        if self.fixed_size:\n",
    "          max_img_len = self.img_segment\n",
    "          n = img_lens[-1]\n",
    "          img_segment = np.pad(img_segment, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        if self.fixed_size:\n",
    "          max_audio_len = self.audio_segment\n",
    "          n = audio_lens[-1]\n",
    "          audio_segment = np.pad(audio_segment, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "        \n",
    "        audios.append(audio_segment)\n",
    "        imgs.append(img_segment)\n",
    "        texts.append(text_segment)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "\n",
    "      res = audios, audio_lens, texts, text_lens, imgs, img_lens, ys_int\n",
    "      return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10262 3069 559\n",
      "1026 306 55\n",
      "0 0 0\n",
      "1026 306 55\n",
      "2052 612 110\n",
      "3078 918 165\n",
      "4104 1224 220\n",
      "5130 1530 275\n",
      "6156 1836 330\n",
      "7182 2142 385\n",
      "8208 2448 440\n",
      "9234 2754 495\n",
      "10260 3060 550\n",
      "11286 3366 605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- hap\n",
      "F1: 0.9108910891089109 \n",
      "A: 0.8368580060422961 \n",
      "WA: 0.5058368399619927\n",
      "276 52 2 1\n",
      "--- sad\n",
      "F1: 0.1913875598086124 \n",
      "A: 0.48942598187311176 \n",
      "WA: 0.5097005637308734\n",
      "20 15 154 142\n",
      "--- fea\n",
      "F1: 0.0 \n",
      "A: 0.7220543806646526 \n",
      "WA: 0.5\n",
      "0 0 92 239\n",
      "--- sur\n",
      "F1: 0.4444444444444445 \n",
      "A: 0.6978851963746223 \n",
      "WA: 0.6131089990581385\n",
      "40 38 62 191\n",
      "--- ang\n",
      "F1: 0.23188405797101444 \n",
      "A: 0.6797583081570997 \n",
      "WA: 0.5455779514603044\n",
      "16 12 94 209\n",
      "--- dis\n",
      "F1: 0.5662100456621004 \n",
      "A: 0.7129909365558912 \n",
      "WA: 0.6851915626345243\n",
      "62 56 39 174\n",
      "0.3908028661658471 0.5599026528076388\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1686, 759, 336, 176, 497, 332]\n",
    "w2 = [max(1400/x, 1) for x in w]\n",
    "# w2 = [max(w)/x for x in w]\n",
    "w3 = w2 / np.linalg.norm(w2, 1)\n",
    "[round(x,3) for x in w2]\n",
    "# w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collates():\n",
    "    audio_segment = 30\n",
    "    audio_step = 15\n",
    "    img_segment = 30\n",
    "    img_step = 15\n",
    "    fixed_size = False\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "      audios, texts, imgs, ys_int = [list(x) for x in zip(*batch)]\n",
    "    \n",
    "#       audio_mean = np.array([audio.mean(0) for audio in audios])\n",
    "#       img_mean = np.array([img.mean(0) for img in imgs])\n",
    "    \n",
    "      audio_len = []\n",
    "      for i, audio in enumerate(audios):\n",
    "        start_id = np.random.randint(0,max(1,audio.shape[0] - self.audio_step*self.audio_segment + 1))\n",
    "        start_id = np.random.randint(0,audio.shape[0])\n",
    "        audios[i] = audio[start_id:start_id + self.audio_step*self.audio_segment:self.audio_step]\n",
    "        \n",
    "      for i, img in enumerate(imgs):\n",
    "#         start_id = np.random.randint(0, max(img.shape[0] - self.img_step*self.img_segment + 1, 1))\n",
    "        start_id = np.random.randint(0, img.shape[0])\n",
    "        imgs[i] = img[start_id:start_id + self.img_step*self.img_segment:self.img_step]\n",
    "\n",
    "#       text_lens = [text.shape[0] for text in texts]\n",
    "#       max_text_len = max(text_lens)\n",
    "#       for i,text in enumerate(texts):\n",
    "#         n = text_lens[i]\n",
    "#         texts[i] = np.pad(text, ((0,max_text_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "      text_lens = None\n",
    "\n",
    "      img_lens = [img.shape[0] for img in imgs]\n",
    "      if self.fixed_size:\n",
    "        max_img_len = self.img_segment\n",
    "      else:\n",
    "        max_img_len = max(img_lens)\n",
    "      for i,img in enumerate(imgs):\n",
    "        n = img_lens[i]\n",
    "        imgs[i] = np.pad(img, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "      audio_lens = [audio.shape[0] for audio in audios]\n",
    "      if self.fixed_size:\n",
    "        max_audio_len = self.audio_segment\n",
    "      else:\n",
    "        max_audio_len = max(audio_lens)\n",
    "      for i,audio in enumerate(audios):\n",
    "        n = audio_lens[i]\n",
    "        audios[i] = np.pad(audio, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        \n",
    "#       audio_mean = np.repeat(np.expand_dims(audio_mean, 1), max_audio_len, 1)\n",
    "#       img_mean = np.repeat(np.expand_dims(img_mean, 1), max_img_len, 1)\n",
    "        \n",
    "      audios = np.stack(audios)\n",
    "#       audios = np.concatenate([audios, audio_mean], -1)\n",
    "      imgs = np.stack(imgs)\n",
    "#       imgs = np.concatenate([imgs, img_mean], -1)\n",
    "\n",
    "        \n",
    "      res = audios, audio_lens, \\\n",
    "        np.concatenate(texts), \\\n",
    "        imgs, img_lens, np.array(ys_int).astype('float32')\n",
    "      return res\n",
    "\n",
    "    def val_collate_fn(self, batch):\n",
    "      entry = batch[0]\n",
    "      audio = entry[0]\n",
    "      text = entry[1]\n",
    "      img = entry[2]\n",
    "      ys_int = np.array([entry[3]]).astype('float32')\n",
    "\n",
    "      audios, imgs = [], []\n",
    "      img_lens, audio_lens = [], []\n",
    "      audio_n = audio.shape[0]\n",
    "      img_n = img.shape[0]\n",
    "        \n",
    "#       audio_mean = audio.mean(0)\n",
    "#       img_mean = img.mean(0)\n",
    "\n",
    "\n",
    "      i = 0\n",
    "      while True:\n",
    "        audio_offset = i*self.audio_step*100\n",
    "        img_offset = i*self.img_step*100\n",
    "\n",
    "        if audio_offset >= audio_n and img_offset >= img_n:\n",
    "            break\n",
    "\n",
    "        if audio_offset < audio_n:\n",
    "          audio_segment = audio[audio_offset:audio_offset + self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(1,audio_n - self.audio_step*self.audio_segment + 1))\n",
    "          start_id = np.random.randint(0, audio_n)\n",
    "          audio_segment = audio[start_id:start_id+self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "\n",
    "        if img_offset < img_n:\n",
    "            img_segment = img[img_offset:img_offset + self.img_step*self.img_segment:self.img_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(img_n - self.img_step*self.img_segment + 1, 1))\n",
    "          start_id = np.random.randint(0, img_n)\n",
    "          img_segment = img[start_id:start_id+self.img_step*self.img_segment:self.img_step, :]\n",
    "        \n",
    "#         audio_segment = np.concatenate([audio, np.repeat(np.expand_dims(audio_mean, 0), audio.shape[0], 0)], -1)\n",
    "#         img_segment = np.concatenate([img, np.repeat(np.expand_dims(img_mean, 0), img.shape[0], 0)], -1)\n",
    "    \n",
    "        img_lens.append(img_segment.shape[0])\n",
    "        audio_lens.append(audio_segment.shape[0])\n",
    "        \n",
    "        \n",
    "        if self.fixed_size:\n",
    "          max_img_len = self.img_segment\n",
    "          n = img_lens[-1]\n",
    "          img_segment = np.pad(img_segment, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        if self.fixed_size:\n",
    "          max_audio_len = self.audio_segment\n",
    "          n = audio_lens[-1]\n",
    "          audio_segment = np.pad(audio_segment, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "        \n",
    "        audios.append(audio_segment)\n",
    "        imgs.append(img_segment)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "#       text_lens = [text.shape[1]]\n",
    "      text_lens = None\n",
    "\n",
    "      res = audios, audio_lens, text, imgs, img_lens, ys_int\n",
    "      return res\n",
    "\n",
    "    def test_collate_fn(self, batch):\n",
    "      entry = batch[0]\n",
    "      audio = entry[0]\n",
    "      text = np.expand_dims(entry[1],0)\n",
    "      img = entry[2]\n",
    "\n",
    "      audios, imgs = [], []\n",
    "      img_lens, audio_lens = [], []\n",
    "      audio_n = audio.shape[0]\n",
    "      img_n = img.shape[0]\n",
    "\n",
    "      i = 0\n",
    "      while True:\n",
    "        audio_offset = i*self.audio_step//3\n",
    "        img_offset = i*self.img_step//3\n",
    "\n",
    "        if audio_offset >= audio_n and img_offset >= img_n:\n",
    "            break\n",
    "\n",
    "        if audio_offset < audio_n:\n",
    "          audio_segment = audio[audio_offset:audio_offset + self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(1,audio_n - self.audio_step*self.audio_segment + 1))\n",
    "          start_id = np.random.randint(0, audio_n)\n",
    "          audio_segment = audio[start_id:start_id+self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "\n",
    "        if img_offset < img_n:\n",
    "            img_segment = img[img_offset:img_offset + self.img_step*self.img_segment:self.img_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(img_n - self.img_step*self.img_segment + 1, 1))\n",
    "          start_id = np.random.randint(0, img_n)\n",
    "          img_segment = img[start_id:start_id+self.img_step*self.img_segment:self.img_step, :]\n",
    "        \n",
    "#         audio_segment = np.concatenate([audio, np.repeat(np.expand_dims(audio_mean, 0), audio.shape[0], 0)], -1)\n",
    "#         img_segment = np.concatenate([img, np.repeat(np.expand_dims(img_mean, 0), img.shape[0], 0)], -1)\n",
    "    \n",
    "        img_lens.append(img_segment.shape[0])\n",
    "        audio_lens.append(audio_segment.shape[0])\n",
    "        \n",
    "        \n",
    "        if self.fixed_size:\n",
    "          max_img_len = self.img_segment\n",
    "          n = img_lens[-1]\n",
    "          img_segment = np.pad(img_segment, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        if self.fixed_size:\n",
    "          max_audio_len = self.audio_segment\n",
    "          n = audio_lens[-1]\n",
    "          audio_segment = np.pad(audio_segment, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "        \n",
    "        audios.append(audio_segment)\n",
    "        imgs.append(img_segment)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "      text_lens = [text.shape[1]]\n",
    "\n",
    "      res = audios, audio_lens, text, text_lens, imgs, img_lens\n",
    "      return res\n",
    "collates = Collates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collates():\n",
    "    audio_segment = 30\n",
    "    audio_step = 15\n",
    "    img_segment = 30\n",
    "    img_step = 15\n",
    "    fixed_size = False\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "      audios, texts, imgs, ys_int = [list(x) for x in zip(*batch)]\n",
    "    \n",
    "#       audio_mean = np.array([audio.mean(0) for audio in audios])\n",
    "#       img_mean = np.array([img.mean(0) for img in imgs])\n",
    "    \n",
    "      audio_len = []\n",
    "      for i, audio in enumerate(audios):\n",
    "        start_id = np.random.randint(0,max(1,audio.shape[0] - self.audio_step*self.audio_segment + 1))\n",
    "        start_id = np.random.randint(0,audio.shape[0])\n",
    "        audios[i] = audio[start_id:start_id + self.audio_step*self.audio_segment:self.audio_step]\n",
    "        \n",
    "      for i, img in enumerate(imgs):\n",
    "#         start_id = np.random.randint(0, max(img.shape[0] - self.img_step*self.img_segment + 1, 1))\n",
    "        start_id = np.random.randint(0, img.shape[0])\n",
    "        imgs[i] = img[start_id:start_id + self.img_step*self.img_segment:self.img_step]\n",
    "\n",
    "#       text_lens = [text.shape[0] for text in texts]\n",
    "#       max_text_len = max(text_lens)\n",
    "#       for i,text in enumerate(texts):\n",
    "#         n = text_lens[i]\n",
    "#         texts[i] = np.pad(text, ((0,max_text_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "      text_lens = None\n",
    "\n",
    "      img_lens = [img.shape[0] for img in imgs]\n",
    "      if self.fixed_size:\n",
    "        max_img_len = self.img_segment\n",
    "      else:\n",
    "        max_img_len = max(img_lens)\n",
    "      for i,img in enumerate(imgs):\n",
    "        n = img_lens[i]\n",
    "        imgs[i] = np.pad(img, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "      audio_lens = [audio.shape[0] for audio in audios]\n",
    "      if self.fixed_size:\n",
    "        max_audio_len = self.audio_segment\n",
    "      else:\n",
    "        max_audio_len = max(audio_lens)\n",
    "      for i,audio in enumerate(audios):\n",
    "        n = audio_lens[i]\n",
    "        audios[i] = np.pad(audio, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        \n",
    "#       audio_mean = np.repeat(np.expand_dims(audio_mean, 1), max_audio_len, 1)\n",
    "#       img_mean = np.repeat(np.expand_dims(img_mean, 1), max_img_len, 1)\n",
    "        \n",
    "      audios = np.stack(audios)\n",
    "#       audios = np.concatenate([audios, audio_mean], -1)\n",
    "      imgs = np.stack(imgs)\n",
    "#       imgs = np.concatenate([imgs, img_mean], -1)\n",
    "\n",
    "        \n",
    "      res = audios, audio_lens, \\\n",
    "        np.concatenate(texts), \\\n",
    "        imgs, img_lens, np.array(ys_int)\n",
    "      return res\n",
    "\n",
    "    def val_collate_fn(self, batch):\n",
    "      entry = batch[0]\n",
    "      audio = entry[0]\n",
    "      text = entry[1]\n",
    "      img = entry[2]\n",
    "      ys_int = np.array([entry[3]])\n",
    "\n",
    "      audios, imgs = [], []\n",
    "      img_lens, audio_lens = [], []\n",
    "      audio_n = audio.shape[0]\n",
    "      img_n = img.shape[0]\n",
    "        \n",
    "#       audio_mean = audio.mean(0)\n",
    "#       img_mean = img.mean(0)\n",
    "\n",
    "\n",
    "      i = 0\n",
    "      while True:\n",
    "        audio_offset = i*self.audio_step\n",
    "        img_offset = i*self.img_step\n",
    "\n",
    "        if audio_offset >= audio_n and img_offset >= img_n:\n",
    "            break\n",
    "\n",
    "        if audio_offset < audio_n:\n",
    "          audio_segment = audio[audio_offset:audio_offset + self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(1,audio_n - self.audio_step*self.audio_segment + 1))\n",
    "          start_id = np.random.randint(0, audio_n)\n",
    "          audio_segment = audio[start_id:start_id+self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "\n",
    "        if img_offset < img_n:\n",
    "            img_segment = img[img_offset:img_offset + self.img_step*self.img_segment:self.img_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(img_n - self.img_step*self.img_segment + 1, 1))\n",
    "          start_id = np.random.randint(0, img_n)\n",
    "          img_segment = img[start_id:start_id+self.img_step*self.img_segment:self.img_step, :]\n",
    "        \n",
    "#         audio_segment = np.concatenate([audio, np.repeat(np.expand_dims(audio_mean, 0), audio.shape[0], 0)], -1)\n",
    "#         img_segment = np.concatenate([img, np.repeat(np.expand_dims(img_mean, 0), img.shape[0], 0)], -1)\n",
    "    \n",
    "        img_lens.append(img_segment.shape[0])\n",
    "        audio_lens.append(audio_segment.shape[0])\n",
    "        \n",
    "        \n",
    "        if self.fixed_size:\n",
    "          max_img_len = self.img_segment\n",
    "          n = img_lens[-1]\n",
    "          img_segment = np.pad(img_segment, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        if self.fixed_size:\n",
    "          max_audio_len = self.audio_segment\n",
    "          n = audio_lens[-1]\n",
    "          audio_segment = np.pad(audio_segment, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "        \n",
    "        audios.append(audio_segment)\n",
    "        imgs.append(img_segment)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "#       text_lens = [text.shape[1]]\n",
    "      text_lens = None\n",
    "\n",
    "      res = audios, audio_lens, text, imgs, img_lens, ys_int\n",
    "      return res\n",
    "\n",
    "    def test_collate_fn(self, batch):\n",
    "      entry = batch[0]\n",
    "      audio = entry[0]\n",
    "      text = np.expand_dims(entry[1],0)\n",
    "      img = entry[2]\n",
    "\n",
    "      audios, imgs = [], []\n",
    "      img_lens, audio_lens = [], []\n",
    "      audio_n = audio.shape[0]\n",
    "      img_n = img.shape[0]\n",
    "\n",
    "      i = 0\n",
    "      while True:\n",
    "        audio_offset = i*self.audio_step//3\n",
    "        img_offset = i*self.img_step//3\n",
    "\n",
    "        if audio_offset >= audio_n and img_offset >= img_n:\n",
    "            break\n",
    "\n",
    "        if audio_offset < audio_n:\n",
    "          audio_segment = audio[audio_offset:audio_offset + self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(1,audio_n - self.audio_step*self.audio_segment + 1))\n",
    "          start_id = np.random.randint(0, audio_n)\n",
    "          audio_segment = audio[start_id:start_id+self.audio_step*self.audio_segment:self.audio_step, :]\n",
    "\n",
    "        if img_offset < img_n:\n",
    "            img_segment = img[img_offset:img_offset + self.img_step*self.img_segment:self.img_step, :]\n",
    "        else:\n",
    "#           start_id = np.random.randint(0, max(img_n - self.img_step*self.img_segment + 1, 1))\n",
    "          start_id = np.random.randint(0, img_n)\n",
    "          img_segment = img[start_id:start_id+self.img_step*self.img_segment:self.img_step, :]\n",
    "        \n",
    "#         audio_segment = np.concatenate([audio, np.repeat(np.expand_dims(audio_mean, 0), audio.shape[0], 0)], -1)\n",
    "#         img_segment = np.concatenate([img, np.repeat(np.expand_dims(img_mean, 0), img.shape[0], 0)], -1)\n",
    "    \n",
    "        img_lens.append(img_segment.shape[0])\n",
    "        audio_lens.append(audio_segment.shape[0])\n",
    "        \n",
    "        \n",
    "        if self.fixed_size:\n",
    "          max_img_len = self.img_segment\n",
    "          n = img_lens[-1]\n",
    "          img_segment = np.pad(img_segment, ((0,max_img_len-n),(0,0)), mode='wrap')\n",
    "\n",
    "        if self.fixed_size:\n",
    "          max_audio_len = self.audio_segment\n",
    "          n = audio_lens[-1]\n",
    "          audio_segment = np.pad(audio_segment, ((0,max_audio_len-n),(0,0)), mode='wrap')\n",
    "        \n",
    "        audios.append(audio_segment)\n",
    "        imgs.append(img_segment)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "\n",
    "      text_lens = [text.shape[1]]\n",
    "\n",
    "      res = audios, audio_lens, text, text_lens, imgs, img_lens\n",
    "      return res\n",
    "collates = Collates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "        collate_fn = collates.val_collate_fn, num_workers=0)\n",
    "batch = batchify(next(iter(val_dataloader))[:6], device)\n",
    "[x.shape for x in batch if x is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=3, shuffle=True,\n",
    "        collate_fn = collates.collate_fn, num_workers=0)\n",
    "batch = next(iter(train_dataloader))\n",
    "batch[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_path = '/usr/cs/public/mohd/data/test1'\n",
    "test1_txt = '/usr/cs/public/mohd/test1_data.txt'\n",
    "test1_dataset = MyDataset(test1_path, test1_txt, mode='test', size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for set_id in range(1,4):\n",
    "    test_path = '/usr/cs/public/mohd/data/test%d'%set_id\n",
    "    test_txt = '/usr/cs/public/mohd/test%d_data.txt'%set_id\n",
    "    test_dataset = MyDataset(test_path, test_txt, mode='test', size=-1)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
    "            collate_fn = collates.test_collate_fn, num_workers=0)\n",
    "    test_iter = iter(test_dataloader)\n",
    "    from data import emotions\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    out_file = open('test%d.csv'%set_id, 'w')\n",
    "    out_file.write(\"FileID,Emotion\\n\")\n",
    "\n",
    "    for i in range(len(test_iter)):\n",
    "        batch = next(test_iter, None)\n",
    "        fn = test_dataset.files[i][:-2]\n",
    "\n",
    "        pred = model(batchify(batch[:6], device)) \n",
    "        pred = pred.mean(0).view(1,-1)\n",
    "        pred = torch.argmax(pred, 1)\n",
    "        pred = emotions[pred]\n",
    "        out_file.write(\"%s,%s\\n\"%(fn, pred))\n",
    "\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([x.shape[0] for x in train_dataset.audios])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model, importlib\n",
    "importlib.reload(model)\n",
    "from model import MyModel2\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "        collate_fn = collates.collate_fn, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "        collate_fn = collates.val_collate_fn, num_workers=0)\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model2 = MyModel2(2,2,200).to(device)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_x = []\n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model2.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
    "# optimizer = optim.Adadelta(model2.parameters(), lr=1)\n",
    "# decay = optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n",
    "while True:\n",
    "    model2.train()\n",
    "    train_iter = iter(train_dataloader)\n",
    "    batch = next(train_iter, None)\n",
    "    if batch is None:\n",
    "        train_iter = iter(train_dataloader)\n",
    "        continue\n",
    "    text = batch[2]\n",
    "    text_len = batch[3]\n",
    "    y = batch[8]\n",
    "    y = torch.tensor(y).to(device)\n",
    "    text = torch.tensor(text).to(device)\n",
    "    text_len = torch.tensor(text_len).to(device)\n",
    "    res = model2(text, text_len)\n",
    "    loss = crit(res, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "    train_losses.append(loss.item())\n",
    "    if model2.iter % 100 == 0:\n",
    "        print('%7s %.2f %.2f'%('[Train]',loss.item(), acc))\n",
    "        plt.plot(train_losses, label='train')\n",
    "#         plt.title('train')   \n",
    "    model2.iter += 1\n",
    "#     decay.step()\n",
    "    \n",
    "    \n",
    "    if model2.iter % 100 == 0:\n",
    "        model2.eval()\n",
    "        val_iter = iter(val_dataloader)\n",
    "        losses, accs= [], []\n",
    "        for i in range(128):\n",
    "            batch = next(val_iter, None)\n",
    "            text = batch[1]\n",
    "            text = batch[2]\n",
    "            text_len = batch[3]\n",
    "            y = batch[8]\n",
    "            y = torch.tensor(y).to(device)\n",
    "            text = torch.tensor(text).to(device)\n",
    "            text_len = torch.tensor(text_len).to(device)\n",
    "            with torch.no_grad():\n",
    "                res = model2(text, text_len)\n",
    "                loss = crit(res, y)\n",
    "            acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "        val_losses.append(mean(losses))\n",
    "        val_x.append(model2.iter)\n",
    "        print('%7s %.2f %.2f'%('[Val]', mean(losses), mean(accs)))\n",
    "        plt.plot(val_x,val_losses, label='val')\n",
    "    if model2.iter % 100 == 0:\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model, importlib\n",
    "importlib.reload(model)\n",
    "from model import MyModel2\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "        collate_fn = collates.collate_fn, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "        collate_fn = collates.val_collate_fn, num_workers=0)\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "model2 = MyModel2(2,2,400).to(device)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_x = []\n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model2.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
    "# optimizer = optim.Adadelta(model2.parameters(), lr=1)\n",
    "# decay = optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n",
    "decay = optim.lr_scheduler.StepLR(optimizer, 1500)\n",
    "print('starting')\n",
    "while True:\n",
    "    model2.train()\n",
    "    train_iter = iter(train_dataloader)\n",
    "    batch = next(train_iter, None)\n",
    "    if batch is None:\n",
    "        train_iter = iter(train_dataloader)\n",
    "        continue\n",
    "    text = batch[2]\n",
    "    text_len = batch[3]\n",
    "    y = batch[8]\n",
    "    y = torch.tensor(y).to(device)\n",
    "    text = torch.tensor(text).to(device)\n",
    "    text_len = torch.tensor(text_len).to(device)\n",
    "    res = model2(text, text_len)\n",
    "    loss = crit(res, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "    train_losses.append(loss.item())\n",
    "    if model2.iter % 100 == 0:\n",
    "        print('%7s %.2f %.2f'%('[Train]',loss.item(), acc))\n",
    "        plt.plot(train_losses, label='train')\n",
    "#         plt.title('train')   \n",
    "    model2.iter += 1\n",
    "#     decay.step()\n",
    "    \n",
    "    \n",
    "    if model2.iter % 100 == 0:\n",
    "        model2.eval()\n",
    "        val_iter = iter(val_dataloader)\n",
    "        losses, accs= [], []\n",
    "        for i in range(128):\n",
    "            batch = next(val_iter, None)\n",
    "            text = batch[1]\n",
    "            text = batch[2]\n",
    "            text_len = batch[3]\n",
    "            y = batch[8]\n",
    "            y = torch.tensor(y).to(device)\n",
    "            text = torch.tensor(text).to(device)\n",
    "            text_len = torch.tensor(text_len).to(device)\n",
    "            with torch.no_grad():\n",
    "                res = model2(text, text_len)\n",
    "                loss = crit(res, y)\n",
    "            acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "        val_losses.append(mean(losses))\n",
    "        val_x.append(model2.iter)\n",
    "        print('%7s %.2f %.2f'%('[Val]', mean(losses), mean(accs)))\n",
    "        plt.plot(val_x,val_losses, label='val')\n",
    "    if model2.iter % 100 == 0:\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model, importlib\n",
    "importlib.reload(model)\n",
    "from model import MyModel2\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True,\n",
    "        collate_fn = collates.collate_fn, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "        collate_fn = collates.val_collate_fn, num_workers=0)\n",
    "device = torch.device('cuda')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "# model2 = MyModel2(2,2,400).to(device)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_x = []\n",
    "\n",
    "crit = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model2.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "# optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
    "# decay = optim.lr_scheduler.ExponentialLR(optimizer, 0.995)\n",
    "decay = optim.lr_scheduler.StepLR(optimizer, 15000)\n",
    "print('starting')\n",
    "while True:\n",
    "    model2.train()\n",
    "    train_iter = iter(train_dataloader)\n",
    "    batch = next(train_iter, None)\n",
    "    if batch is None:\n",
    "        train_iter = iter(train_dataloader)\n",
    "        continue\n",
    "    text = batch[2]\n",
    "    text_len = batch[3]\n",
    "    y = batch[8]\n",
    "    y = torch.tensor(y).to(device)\n",
    "    text = torch.tensor(text).to(device)\n",
    "    text_len = torch.tensor(text_len).to(device)\n",
    "    res = model2(text, text_len)\n",
    "    loss = crit(res, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "    train_losses.append(loss.item())\n",
    "    if model2.iter % 500 == 0:\n",
    "        print('%7s %.2f %.2f'%('[Train]',loss.item(), acc))\n",
    "        plt.plot(train_losses, label='train')\n",
    "#         plt.title('train')   \n",
    "    model2.iter += 1\n",
    "    decay.step()\n",
    "    \n",
    "    \n",
    "    if model2.iter % 500 == 0:\n",
    "        model2.eval()\n",
    "        val_iter = iter(val_dataloader)\n",
    "        losses, accs= [], []\n",
    "        for i in range(512):\n",
    "            batch = next(val_iter, None)\n",
    "            text = batch[1]\n",
    "            text = batch[2]\n",
    "            text_len = batch[3]\n",
    "            y = batch[8]\n",
    "            y = torch.tensor(y).to(device)\n",
    "            text = torch.tensor(text).to(device)\n",
    "            text_len = torch.tensor(text_len).to(device)\n",
    "            with torch.no_grad():\n",
    "                res = model2(text, text_len)\n",
    "                loss = crit(res, y)\n",
    "            acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "            losses.append(loss.item())\n",
    "            accs.append(acc)\n",
    "        val_losses.append(mean(losses))\n",
    "        val_x.append(model2.iter)\n",
    "        print('%7s %.2f %.2f'%('[Val]', mean(losses), mean(accs)))\n",
    "        plt.plot(val_x,val_losses, label='val')\n",
    "    if model2.iter % 500 == 0:\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "collates.audio_segment = 50\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True,\n",
    "        collate_fn = collates.collate_fn, num_workers=0)\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.iter = 0\n",
    "#         self.rnn = nn.LSTM(80, 256, batch_first=True)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(80,16,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16,8,3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "#         self.lin = nn.Sequential(\n",
    "#             nn.Linear(256, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32,256),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "        self.classify = nn.Linear(256, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         _, (h, _) = self.rnn(x)\n",
    "#         h = torch.cat([x for x in h], -1)\n",
    "#         h = self.lin(h)\n",
    "        h = self.cnn(x)\n",
    "        print(h.shape)\n",
    "        return self.classify(h)\n",
    "    \n",
    "model = TestModel().to(device)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model2.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "while True:\n",
    "    model.train()\n",
    "    train_iter = iter(train_dataloader)\n",
    "    batch = next(train_iter, None)\n",
    "    if batch is None:\n",
    "        train_iter = iter(train_dataloader)\n",
    "        continue\n",
    "    audio = batch[0]\n",
    "    y = batch[7]\n",
    "    y = torch.tensor(y).to(device)\n",
    "    audio = torch.tensor(audio).to(device)\n",
    "    res = model(audio)\n",
    "    loss = crit(res, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "    if model.iter % 20 == 0:\n",
    "        print('\\r%7s %.2f %.2f'%('[Train]',loss.item(), acc), end='')\n",
    "    model.iter += 1\n",
    "    \n",
    "#     if model2.iter % 50 == 0:\n",
    "#         model2.eval()\n",
    "#         val_iter = iter(val_dataloader)\n",
    "#         losses, accs= [], []\n",
    "#         for i in range(512):\n",
    "#             batch = next(val_iter, None)\n",
    "#             text = batch[1]\n",
    "#             y = batch[7]\n",
    "#             y = torch.tensor(y).to(device)\n",
    "#             text = torch.tensor(text).to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 res = model2(text)\n",
    "#                 loss = crit(res, y)\n",
    "#             acc = (torch.argmax(res,1) == y).cpu().numpy().mean()\n",
    "#             losses.append(loss.item())\n",
    "#             accs.append(acc)\n",
    "#         print('\\n%7s %.2f %.2f'%('[Val]', mean(losses), mean(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs1 = [32,64,128,256,512]\n",
    "hs2 = [32,64,128]\n",
    "lrs = [1e-2,1e-3,1e-3,1e-4]\n",
    "regs = [1e-4,1e-5,1e-6,1e-6]\n",
    "segments = [5,10,15,20,25,30,35,40,45,50]\n",
    "steps = [10,20,30,40,50]\n",
    "BS = [64,128,256,512]\n",
    "\n",
    "for i in range(1000):\n",
    "    H = np.random.choice(hs1, 7)\n",
    "    H2 = np.random.choice(hs2, 1)\n",
    "    H[2] = H2[0]\n",
    "    S = np.random.choice(segments, 2)\n",
    "    ST = np.random.choice(steps, 2)\n",
    "    bs = np.random.choice(BS)\n",
    "#     reg = np.random.choice(regs, 4)\n",
    "#     lr = np.random.choice(lrs, 4)\n",
    "    \n",
    "    \n",
    "#     lr = [float(x) for x in lr]\n",
    "#     reg = [float(x) for x in reg]\n",
    "#     bs = int(bs)\n",
    "    H = [int(x) for x in H]\n",
    "    S = [int(x) for x in S]\n",
    "    ST = [int(x) for x in ST]\n",
    "    bs = int(bs)\n",
    "#     BN = [int(x) for x in BN]\n",
    "#     L = [int(x) for x in L]\n",
    "    \n",
    "        \n",
    "    collates.audio_segment = S[0]\n",
    "    collates.audio_step = ST[0]\n",
    "    collates.img_segment = S[1]\n",
    "    collates.img_step = ST[1]\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True,\n",
    "            collate_fn = collates.collate_fn, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "            collate_fn = collates.val_collate_fn, num_workers=0)\n",
    "\n",
    "    hparams = HParams(audio_h = H[0], img_h = H[1], text_h = H[2],\n",
    "                      fc1_av = H[3], fc1_at = H[4], fc1_vt = H[5], fc2_dim = H[6],\n",
    "                      epoch_size = len(train_dataloader))\n",
    "\n",
    "\n",
    "    cur_datetime = datetime.now().strftime(\"%m-%d-%H:%M:%S\")\n",
    "    name = \"H=[{}]-img=[{} {}]-audio=[{} {}]-bs=[{}]\".format(\" \".join(map(str,H)),\n",
    "                                                     collates.img_segment, collates.img_step,\n",
    "                                                     collates.audio_segment, collates.audio_step, bs)\n",
    "    if name:\n",
    "        name = \"%s-%s\"%(cur_datetime,name)\n",
    "    else:\n",
    "        name = cur_datetime\n",
    "\n",
    "    ## Model init !!!\n",
    "    model, writer = initiate_model(hparams, name)\n",
    "    optimizers = initiate_optimizers(hparams, model)\n",
    "\n",
    "    train_iter = iter(train_dataloader)\n",
    "    data_counter = 0\n",
    "    if model.hparams.setting == 'aux':\n",
    "        earlystopper_audio = EarlyStopping()\n",
    "        earlystopper_img = EarlyStopping()\n",
    "        earlystopper_text = EarlyStopping()\n",
    "        earlystopper_av = EarlyStopping()\n",
    "        earlystopper_at = EarlyStopping()\n",
    "        earlystopper_vt = EarlyStopping()\n",
    "        earlystopper_int = EarlyStopping()\n",
    "    elif model.hparams.setting == 'full':\n",
    "        earlystopper_stage1 = EarlyStopping(patience=20)\n",
    "        earlystopper_stage2 = EarlyStopping(patience=40)\n",
    "        earlystopper_stage3 = EarlyStopping(patience=60)\n",
    "    earlystopper_spk = EarlyStopping(patience=10)\n",
    "\n",
    "    print(\"[Starting]\", model.name)\n",
    "    while True:\n",
    "        model.train()\n",
    "        batch = next(train_iter, None)\n",
    "        if batch is None:\n",
    "            train_iter = iter(train_dataloader)\n",
    "            batch = next(train_iter, None)\n",
    "\n",
    "\n",
    "    #     batch = list(batch)\n",
    "    #     batch[0] += np.random.normal(0,0.3,batch[0].shape)\n",
    "    #     batch[1] += np.random.normal(0,0.2,batch[1].shape)\n",
    "    #     batch[3] += np.random.normal(0,0.3,batch[3].shape)\n",
    "\n",
    "\n",
    "        batch = [torch.tensor(x).to(device) for x in batch]\n",
    "        res = train_step(model, optimizers, batch, writer)\n",
    "        if model.iter % (hparams.epoch_size//3) == 0:\n",
    "            val_iter = iter(val_dataloader)\n",
    "            model.eval()\n",
    "            val_batch = 256\n",
    "            results = []\n",
    "            for i in range(val_batch):\n",
    "                batch = next(val_iter, None)\n",
    "                if batch is None:\n",
    "                    val_iter = iter(val_dataloader)\n",
    "                    batch = next(val_iter, None)\n",
    "                ys_audio, ys_img, ys_int, spk, gen, age = [torch.tensor(x).to(device) for x in batch[6:]]\n",
    "\n",
    "                preds = model(batchify(batch[:6], device)) \n",
    "                preds = [x.mean(0).view(1,-1) for x in preds]\n",
    "\n",
    "                if model.hparams.setting == 'aux' and (model.stage == 1 or model.stage == 2):\n",
    "                    if model.stage == 1:\n",
    "                        with torch.no_grad():\n",
    "                            loss_audio = model.loss(preds[0], ys_audio).item()\n",
    "                            loss_img = model.loss(preds[1], ys_img).item()\n",
    "                            loss_text = model.loss(preds[2], ys_int).item()\n",
    "                            loss_spk = model.hparams.alpha_spk * model.loss(preds[3], spk).mean()\n",
    "                            loss_gen = model.hparams.alpha_gen * model.loss(preds[4], gen).mean()\n",
    "                            loss_age = model.hparams.alpha_age * model.reg_loss(preds[5], age).mean()\n",
    "                        audio_acc = (torch.argmax(preds[0],1) == ys_audio).cpu().numpy().mean()\n",
    "                        img_acc = (torch.argmax(preds[1],1) == ys_img).cpu().numpy().mean()\n",
    "                        text_acc = (torch.argmax(preds[2],1) == ys_int).cpu().numpy().mean()\n",
    "\n",
    "                        results.append((loss_audio, loss_img, loss_text,\n",
    "                                       audio_acc, img_acc, text_acc,\n",
    "                                       loss_spk, loss_gen, loss_age))\n",
    "                    elif model.stage == 2:\n",
    "                        with torch.no_grad():\n",
    "                            loss_av = model.loss(preds[0], ys_int).item()\n",
    "                            loss_at = model.loss(preds[1], ys_int).item()\n",
    "                            loss_vt = model.loss(preds[2], ys_int).item()\n",
    "                            loss_spk = model.hparams.alpha_spk * model.loss(preds[3], spk).mean()\n",
    "                            loss_gen = model.hparams.alpha_gen * model.loss(preds[4], gen).mean()\n",
    "                            loss_age = model.hparams.alpha_age * model.reg_loss(preds[5], age).mean()\n",
    "\n",
    "                        av_acc = (torch.argmax(preds[0],1) == ys_audio).cpu().numpy().mean()\n",
    "                        at_acc = (torch.argmax(preds[1],1) == ys_img).cpu().numpy().mean()\n",
    "                        vt_acc = (torch.argmax(preds[2],1) == ys_int).cpu().numpy().mean()\n",
    "\n",
    "                        results.append((loss_av, loss_at, loss_vt,\n",
    "                                       av_acc, at_acc, vt_acc,\n",
    "                                       loss_spk, loss_gen, loss_age))\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        loss_int = model.loss(preds[0], ys_int).item()\n",
    "                        loss_spk = model.hparams.alpha_spk * model.loss(preds[1], spk).mean()\n",
    "                        loss_gen = model.hparams.alpha_gen * model.loss(preds[2], gen).mean()\n",
    "                        loss_age = model.hparams.alpha_age * model.reg_loss(preds[3], age).mean()\n",
    "\n",
    "                    int_acc = (torch.argmax(preds[0],1) == ys_int).cpu().numpy().mean()\n",
    "                    results.append((loss_int, int_acc, loss_spk, loss_gen, loss_age))\n",
    "\n",
    "            results = [mean([res[i] for res in results]) for i in range(len(results[0]))]\n",
    "\n",
    "            if model.hparams.setting == 'aux':\n",
    "                if model.stage == 1:\n",
    "                    loss_audio, loss_img, loss_text, audio_acc, img_acc, text_acc, loss_spk, loss_gen, loss_age = results\n",
    "\n",
    "                    if not model.earlystop_audio and earlystopper_audio(loss_audio):\n",
    "                        model.earlystop_audio = True\n",
    "                        print(\"[%d] Stopped audio\"%model.iter)\n",
    "                    if not model.earlystop_img and earlystopper_img(loss_img):\n",
    "                        model.earlystop_img = True\n",
    "                        print(\"[%d] Stopped img\"%model.iter)\n",
    "                    if not model.earlystop_text and earlystopper_text(loss_text):\n",
    "                        model.earlystop_text = True\n",
    "                        print(\"[%d] Stopped text\"%model.iter)\n",
    "                    if earlystopper_spk(loss_spk):\n",
    "                        model.earlystop_spk = True\n",
    "\n",
    "                    if not model.earlystop_audio:\n",
    "                        writer.add_scalar('val/stage1/loss/audio_loss', loss_audio, model.iter)\n",
    "                        writer.add_scalar('val/stage1/acc/audio_acc',audio_acc, model.iter)\n",
    "                    if not model.earlystop_img:\n",
    "                        writer.add_scalar('val/stage1/loss/img_loss', loss_img, model.iter)\n",
    "                        writer.add_scalar('val/stage1/acc/img_acc',img_acc, model.iter)\n",
    "                    if not model.earlystop_text:\n",
    "                        writer.add_scalar('val/stage1/loss/text_loss', loss_text, model.iter)\n",
    "                        writer.add_scalar('val/stage1/acc/text_acc',text_acc, model.iter)\n",
    "\n",
    "                    if model.earlystop_audio and model.earlystop_img and model.earlystop_text:\n",
    "                        model.stage = 2\n",
    "                        model.earlystop_spk = False\n",
    "                        earlystopper_spk.counter = 0\n",
    "                elif model.stage == 2:\n",
    "                    loss_av, loss_at, loss_vt, av_acc, at_acc, vt_acc, loss_spk, loss_gen, loss_age = results\n",
    "\n",
    "                    if not model.earlystop_av and earlystopper_av(loss_av):\n",
    "                        model.earlystop_av = True\n",
    "                        print(\"[%d] Stopped av\"%model.iter)\n",
    "                    if not model.earlystop_at and earlystopper_at(loss_at):\n",
    "                        model.earlystop_at = True\n",
    "                        print(\"[%d] Stopped at\"%model.iter)\n",
    "                    if not model.earlystop_vt and earlystopper_vt(loss_vt):\n",
    "                        model.earlystop_vt = True\n",
    "                        print(\"[%d] Stopped vt\"%model.iter)\n",
    "                    if earlystopper_spk(loss_spk):\n",
    "                        model.earlystop_spk = True\n",
    "\n",
    "                    if not model.earlystop_av:\n",
    "                        writer.add_scalar('val/stage2/loss/av_loss', loss_av, model.iter)\n",
    "                        writer.add_scalar('val/stage2/acc/av_acc',av_acc, model.iter)\n",
    "                    if not model.earlystop_at:\n",
    "                        writer.add_scalar('val/stage2/loss/at_loss', loss_at, model.iter)\n",
    "                        writer.add_scalar('val/stage2/acc/at_acc',at_acc, model.iter)\n",
    "                    if not model.earlystop_vt:\n",
    "                        writer.add_scalar('val/stage2/loss/vt_loss', loss_vt, model.iter)\n",
    "                        writer.add_scalar('val/stage2/acc/vt_acc',vt_acc, model.iter)\n",
    "\n",
    "                    if model.earlystop_av and model.earlystop_at and model.earlystop_vt:\n",
    "                        model.stage = 3\n",
    "                elif model.stage == 3:\n",
    "                    loss_int, int_acc, loss_spk, loss_gen, loss_age = results\n",
    "\n",
    "                    writer.add_scalar('val/stage3/loss/int_loss', loss_int, model.iter)\n",
    "                    writer.add_scalar('val/stage3/acc/int_acc',int_acc, model.iter)\n",
    "\n",
    "\n",
    "                    if earlystopper_int(loss_int):\n",
    "                        print(\"[%d] Stopped int\"%model.iter)\n",
    "                        break\n",
    "\n",
    "            elif model.hparams.setting == 'full':\n",
    "                loss_int, int_acc, loss_spk, loss_gen, loss_age = results\n",
    "                if model.stage == 1:\n",
    "                    if earlystopper_spk(loss_spk):\n",
    "                        model.earlystop_spk = True\n",
    "                    if earlystopper_stage1(loss_int):\n",
    "                        model.stage = 2\n",
    "                        print(\"[%d] Finished stage 1\"%model.iter)\n",
    "                        model.earlystop_spk = False\n",
    "                        earlystopper_spk.counter = 0\n",
    "                elif model.stage == 2:\n",
    "                    if earlystopper_spk(loss_spk):\n",
    "                        model.earlystop_spk = True\n",
    "                    if earlystopper_stage2(loss_int):\n",
    "                        model.stage = 3\n",
    "                        print(\"[%d] Finished stage 2\"%model.iter)\n",
    "                elif model.stage == 3:\n",
    "                    if earlystopper_spk(loss_spk):\n",
    "                        model.earlystop_spk = True\n",
    "                    if earlystopper_stage3(loss_int):\n",
    "                        print(\"[%d] Finished stage 3\"%model.iter)\n",
    "                        break\n",
    "                writer.add_scalar('val/loss/int_loss', loss_int, model.iter)\n",
    "                writer.add_scalar('val/acc/int_acc',int_acc, model.iter)\n",
    "\n",
    "            if not model.earlystop_spk:\n",
    "                writer.add_scalar('val/loss/spk/spk', loss_spk, model.iter)\n",
    "                writer.add_scalar('val/loss/spk/gen', loss_gen, model.iter)\n",
    "                writer.add_scalar('val/loss/spk/age', loss_age, model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from data import emotions\n",
    "import matplotlib.pyplot as plt\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=30, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "for batch in train_dataloader:\n",
    "    imgs = batch[3][:n]\n",
    "    y = [emotions[i] for i in batch[-2]][:n]\n",
    "    imgs = imgs.reshape(-1,2048)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tsne.fit_transform(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = {emo: [] for emo in emotions}\n",
    "offset = 0\n",
    "for label in y:\n",
    "    x_plot[label].append(x[offset:offset+16].mean(0))\n",
    "    offset += 16\n",
    "for label in y:\n",
    "    x_plot[label] = np.array(x_plot[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in emotions:\n",
    "    plt.scatter(x_plot[label][:,0], x_plot[label][:,1], label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob('/usr/cs/public/data/train/*_imgs')\n",
    "paths.sort()\n",
    "path = paths[np.random.randint(9600)]\n",
    "path = paths[9599]\n",
    "print(path)\n",
    "for i,fn in enumerate(os.listdir(path)):\n",
    "    if i % 20 == 0:\n",
    "        im = Image.open(os.path.join(path,fn))\n",
    "        plt.imshow(im)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob('/usr/cs/public/data/train/*_imgs')\n",
    "paths.sort()\n",
    "path = paths[np.random.randint(7500)]\n",
    "t0 = time()\n",
    "ims = []\n",
    "for i,fn in enumerate(os.listdir(path)):\n",
    "    im = Image.open(os.path.join(path,fn))\n",
    "    ims.append(np.array(im))\n",
    "imd = np.stack(ims)\n",
    "print(time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  audios, texts, imgs, ys_audio, ys_img, ys_int = [list(x) for x in zip(*batch)]\n",
    "\n",
    "  text_lens = [text.shape[0] for text in texts]\n",
    "  max_text_len = max(text_lens)\n",
    "  for i,text in enumerate(texts):\n",
    "    n = text_lens[i]\n",
    "    texts[i] = np.pad(text, ((0,max_text_len-n),(0,0)), mode='constant')\n",
    "    \n",
    "  img_lens = [img.shape[0] for img in imgs]\n",
    "  max_img_len = max(img_lens)\n",
    "  for i,img in enumerate(imgs):\n",
    "    n = img_lens[i]\n",
    "    imgs[i] = np.pad(img, ((0,max_img_len-n),(0,0)), mode='constant')\n",
    "\n",
    "\n",
    "  res = np.stack(audios).transpose(0,2,1), np.stack(texts), text_lens, np.stack(imgs), img_lens, \\\n",
    "    np.array(ys_audio), np.array(ys_img), np.array(ys_int)\n",
    "  return res\n",
    "\n",
    "def val_collate_fn(batch):\n",
    "  entry = batch[0]\n",
    "  audio = np.expand_dims(entry[0],0).transpose(0,2,1)\n",
    "  text = np.expand_dims(entry[1],0)\n",
    "  img = np.expand_dims(entry[2],0)\n",
    "  ys_audio = np.array([entry[3]])\n",
    "  ys_img = np.array([entry[4]])\n",
    "  ys_int = np.array([entry[5]])\n",
    "\n",
    "  audios, imgs = [], []\n",
    "  img_lens = []\n",
    "  audio_n = audio.shape[1]\n",
    "  img_n = img.shape[1]\n",
    "  audio_segments = ceil(audio_n/AUDIO_SEGMENT)\n",
    "  img_segments = ceil(img_n/IMG_SEGMENT)\n",
    "\n",
    "  for i in range(max(audio_segments, img_segments)):\n",
    "    audio_offset = i*AUDIO_SEGMENT\n",
    "    img_offset = i*IMG_SEGMENT\n",
    "    if audio_offset < audio_n:\n",
    "      audios.append(audio[:, audio_offset:audio_offset + AUDIO_SEGMENT, :])\n",
    "    else:\n",
    "      start_id = np.random.randint(0, audio_n - AUDIO_SEGMENT + 1)\n",
    "      audios.append(audio[:, start_id:start_id+AUDIO_SEGMENT, :])\n",
    "    if img_offset < img_n:\n",
    "        imgs.append(img[:, img_offset:img_offset + IMG_SEGMENT, :])\n",
    "    else:\n",
    "      start_id = np.random.randint(0, max(img_n - IMG_SEGMENT + 1, 1))\n",
    "      imgs.append(img[:, start_id:start_id+IMG_SEGMENT, :])\n",
    "    img_lens.append(imgs[-1].shape[1])\n",
    "    \n",
    "  text_lens = [text.shape[1]]\n",
    "\n",
    "  res = audios, text, text_lens, imgs, img_lens, ys_audio, ys_img, ys_int\n",
    "  return res\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "        collate_fn = collate_fn, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True,\n",
    "        collate_fn = val_collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from data import emotions\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "val_iter = iter(val_dataloader)\n",
    "model.eval()\n",
    "val_batch = 10\n",
    "predictions = [[], [], []]\n",
    "trues = [[], [], []]\n",
    "for i in range(val_batch):\n",
    "    batch = next(val_iter, None)\n",
    "    if batch is None:\n",
    "        val_iter = iter(val_dataloader)\n",
    "        batch = next(val_iter, None)\n",
    "    audios, text, text_lens, imgs, img_lens, ys_audio, ys_img, ys_int = batch    \n",
    "    trues[0].extend(ys_img.tolist())\n",
    "    text, text_lens, ys_audio, ys_img, ys_int = [torch.tensor(x).to(device) \\\n",
    "                                                 for x in (text, text_lens, ys_audio, ys_img, ys_int)]\n",
    "    preds = [[] for i in range(3)]\n",
    "    print(\"True:\", ys_img.item())\n",
    "    subpreds = []\n",
    "    confs = []\n",
    "    for audio, img, img_len in zip(audios, imgs, img_lens):\n",
    "        audio = torch.tensor(audio).to(device)\n",
    "        img = torch.tensor(img).to(device)\n",
    "        img_len = torch.tensor([img_len]).to(device)\n",
    "        batch = audio, text, text_lens, img, img_len\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch)\n",
    "        x = torch.argmax(pred[1],1).item()\n",
    "        subpreds.append(x)\n",
    "        confs.append(pred[1][0][x].item())\n",
    "        for i in range(len(preds)):\n",
    "            preds[i].append(pred[i].cpu().numpy())\n",
    "    print(\"Pred:\", subpreds)\n",
    "    print(\"Confs\", [\"%.2f\"%x for x in confs])\n",
    "    preds_avg = [combine_avg(ps) for ps in preds]\n",
    "    preds = [torch.tensor(l).to(device) for l in preds_avg]\n",
    "#     int_c = torch.argmax(preds[0],1)\n",
    "    img_c = torch.argmax(preds[1],1)\n",
    "    predictions[0].extend(img_c.cpu().numpy().tolist())    \n",
    "\n",
    "print(emotions)\n",
    "cm = confusion_matrix(trues[0], predictions[0], labels=list(range(7)))\n",
    "print(cm)\n",
    "sns.heatmap(cm,\n",
    "            xticklabels = emotions,\n",
    "            yticklabels = emotions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    for x in batch[0]:\n",
    "        plt.imshow(x.T, origin='lower')\n",
    "        plt.show()\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
